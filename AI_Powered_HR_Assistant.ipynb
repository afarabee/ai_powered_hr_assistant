{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOekv1jRSjc6rB9Xx741zTE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afarabee/ai_powered_hr_assistant/blob/main/AI_Powered_HR_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Instructions\n",
        "**Crafting an AI-Powered HR Assistant: A Use Case for Nestl√©‚Äôs HR Policy Documents**\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Overview  \n",
        "The project aims to create a conversational chatbot that responds to user inquiries using PDF document information. It requires proficiency in:\n",
        "- Extracting and converting text into numerical vectors\n",
        "- Establishing an answer-finding mechanism\n",
        "- Designing a user-friendly chatbot interface with Gradio\n",
        "\n",
        "Additionally, the project emphasizes:\n",
        "- Structuring inquiries for clear communication\n",
        "- Deploying the chatbot for practical use\n",
        "- Guaranteeing the system's accessibility and efficiency in meeting user needs\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Instructions  \n",
        "- Review the learning materials and the Gradio documentation provided for the project  \n",
        "- Read the sections on **situation, task, action, and result** carefully to understand the assignment  \n",
        "- Complete and submit the assignment through the Learning Management System (LMS)  \n",
        "- Adhere closely to the provided guidelines, ensuring your submission contains all necessary analyses and interpretations  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Situation  \n",
        "As a developer, you have received the critical task of improving the operational efficiency of **Nestl√©'s Human Resources department**, a leading multinational corporation.\n",
        "\n",
        "Your toolkit includes:\n",
        "- Conversational AI technology\n",
        "- Python libraries\n",
        "- The powerful **GPT model from OpenAI**\n",
        "- The user-friendly **Gradio UI**\n",
        "\n",
        "Your mission is to integrate these advanced tools to **transform HR processes**, creating a more streamlined and efficient workflow within Nestl√©.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Task  \n",
        "Your task is to develop a **conversational chatbot** that answers queries about Nestl√©'s HR reports efficiently.\n",
        "\n",
        "You must:\n",
        "- Use **Python libraries**, **OpenAI's GPT model**, and **Gradio UI**\n",
        "- Create an interface that extracts and processes information from documents\n",
        "- Provide accurate responses to user queries through the chatbot\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Action Steps  \n",
        "\n",
        "- ‚úÖ Import essential tools and set up OpenAI's API environment  \n",
        "- ‚úÖ Load Nestl√©'s HR policy using `PyPDFLoader` and split it for easy processing  \n",
        "- ‚úÖ Create vector representations for text chunks using **ChromaDB** and **OpenAI's embeddings**  \n",
        "- ‚úÖ Build a question-answering system using the **GPT-3.5 Turbo model** to retrieve answers  \n",
        "- ‚úÖ Create a **prompt template** to guide the chatbot‚Äôs responses  \n",
        "- ‚úÖ Use **Gradio** to build a user-friendly chatbot interface for interaction and information retrieval  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Result  \n",
        "Upon completing this project, you will submit a `.ipynb` file demonstrating your ability to use advanced AI and machine learning technologies to develop a conversational chatbot.\n",
        "\n",
        "Your submission must include:\n",
        "- Setting up the programming environment  \n",
        "- Processing text documents  \n",
        "- Creating vector representations  \n",
        "- Building a question-answering system  \n",
        "- Designing a **Gradio interface** for effective interaction  \n",
        "\n",
        "Ensure the interface is **clear, usable, and accurate** in retrieving relevant information from Nestl√©‚Äôs HR policy.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "GiR0ng62svrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Requirements Breakdown\n",
        "\n",
        "You're building a smart **HR assistant** that can read the **Nestl√© HR policy PDF** and answer employee questions like:\n",
        "- ‚ÄúWhat is Nestl√©‚Äôs policy on promotions?‚Äù\n",
        "- ‚ÄúWhat does Nestl√© offer for employee training?‚Äù\n",
        "\n",
        "This involves:\n",
        "- üóÉÔ∏è Extracting text from the PDF  \n",
        "- üî¢ Converting text chunks into numerical format (embeddings)  \n",
        "- üß† Storing those embeddings in a retrievable way (Chroma DB)  \n",
        "- üí¨ Asking GPT to find answers using those chunks  \n",
        "- üñºÔ∏è Displaying this interaction using Gradio  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lZreg7yi2dOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Set Up Environment\n",
        "- Install required packages (`openai`, `langchain`, `chromadb`, `PyPDFLoader`, `gradio`, etc.)  \n",
        "- Set your OpenAI API key securely using Colab Secrets  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ev-EMDgN12U7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OInV06TArYw8"
      },
      "outputs": [],
      "source": [
        "# Step 2: Setup environment\n",
        "\n",
        "# 2A: Install necessary libraries\n",
        "# openai ‚Üí to connect to GPT-3.5 Turbo via API\n",
        "# pypdf ‚Üí to split up Nestle's HR policy for easier processing, used by LangChain's PDF loader\n",
        "# chromadb ‚Üí use with OpenAI's Embeddings to create vector representations of chunks of text from the PDF that can be stored and searched for\n",
        "# gradio ‚Üí to build user-friendly chatbot interface, enabling interaction and information retrieval\n",
        "# langchain ‚Üí for handling document loading, splitting, and retrieval logic\n",
        "# tiktoken ‚Üí used internally by OpenAI for counting tokens (needed in retrieval chains)\n",
        "\n",
        "!pip install --quiet openai pypdf chromadb gradio langchain tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2B: Access my OpenAI API key securely using Colab Secrets\n",
        "# Avoids typing my key manually or exposing it in the notebook\n",
        "\n",
        "from google.colab import userdata # access my stored keys in Colab\n",
        "from openai import OpenAI         # new OpenAI client class\n",
        "\n",
        "api_key = userdata.get(\"OpenAI\")  # securely fetch my key from Secrets\n",
        "client = OpenAI(api_key=api_key)  # pass it into the OpenAI client directly\n"
      ],
      "metadata": {
        "id": "P57h2l1Tu-4c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2C: Define get_completion function using the updated SDK and secure key\n",
        "\n",
        "from google.colab import userdata               # use Colab's secure key store\n",
        "from openai import OpenAI                       # import the new OpenAI client\n",
        "\n",
        "api_key = userdata.get(\"OpenAI\")                # securely fetch my key from Secrets\n",
        "client = OpenAI(api_key=api_key)                # build a working OpenAI client with that key\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):  # send the prompt using the new SDK style\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content   # extract and return the assistant's reply\n"
      ],
      "metadata": {
        "id": "pd2J2Ca32Hhv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Step 1 E2E\n",
        "\n",
        "get_completion(\"What is the purpose of a human resources policy?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "uxvonduXwpv8",
        "outputId": "6bad0f1c-6589-4453-98e5-9fc8313644a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The purpose of a human resources policy is to provide guidelines and procedures for managing employees in a fair, consistent, and legally compliant manner. These policies help to ensure that employees are treated fairly, that their rights are protected, and that the organization operates in accordance with relevant laws and regulations. Additionally, human resources policies help to establish expectations for employee behavior, performance, and conduct, and provide a framework for resolving conflicts and addressing issues that may arise in the workplace. Overall, human resources policies help to create a positive and productive work environment for employees and support the organization in achieving its goals and objectives.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load and Split the PDF\n",
        "Use LangChain‚Äôs PyPDFLoader to extract text from the Nestl√© HR policy. Then, break the content into smaller chunks.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "N66x6P7D1tSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3A: Install addtl requiredLangChain package\n",
        "!pip install --quiet langchain langchain-community"
      ],
      "metadata": {
        "id": "UZLD3LJn89J6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3B: Import LangChain's PDF loader and text splitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "ZSkhmKPT6sGU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "local file location: file:///C:/Users/aimee/OneDrive/Documents/AppliedGenAI/Building%20LLM%20Applications/Assessment_1/1728286846_the_nestle_hr_policy_pdf_2012.pdf"
      ],
      "metadata": {
        "id": "xSH5KLLK7K-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3C: Upload HR policy PDF to Colab environment to get the correct path\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "utaSAw047PNf",
        "outputId": "4460c8ee-b9b8-468d-9da0-151d1b0ee8c5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d8a5bd80-bbe1-456f-8bcb-a60fe4d46fdf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d8a5bd80-bbe1-456f-8bcb-a60fe4d46fdf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1728286846_the_nestle_hr_policy_pdf_2012.pdf to 1728286846_the_nestle_hr_policy_pdf_2012 (2).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3D: Load the Nestl√© HR policy document and define the path\n",
        "pdf_path = \"/content/1728286846_the_nestle_hr_policy_pdf_2012.pdf\"  ### update path if needed ###\n",
        "\n",
        "# Use PyPDFLoader from LangChain to extract PDF content\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "\n",
        "# Step 3B: Extract the document and return a list of LangChain Document objects\n",
        "# Each page will become a separate document object (1 per PDF pg)\n",
        "pages = loader.load()\n",
        "\n",
        "# Test: Check how many pages were loaded\n",
        "print(f\"Loaded {len(pages)} pages from the PDF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGV0_4d76z5_",
        "outputId": "d8e96fde-ca29-4d00-95a4-1cc161f9231c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 8 pages from the PDF.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3E: Define how to chunk the text\n",
        "# I want chunks that are not too long, and that overlap slightly to preserve context\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,       # max characters per chunk (adjustable)\n",
        "    chunk_overlap=100     # overlap to keep context from spilling over boundaries\n",
        ")\n",
        "\n",
        "# Test: Text_splitter was created successfully\n",
        "print(type(text_splitter))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8OTAxM39saA",
        "outputId": "a0738f52-036d-450a-f6e9-201c1b73119d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_text_splitters.character.RecursiveCharacterTextSplitter'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3F: Split all pages into smaller chunks\n",
        "# This will give me a list of Document chunks, ready to be embedded later\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "# Test: Preview the first chunk to see how it looks\n",
        "print(chunks[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2FKO99s_gCV",
        "outputId": "a6c64cdb-287b-4367-e15a-b4666b1b1594"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy\n",
            "Mandatory\n",
            "September‚Äâ‚Äâ2012\n",
            "The Nestl√©  \n",
            "Human Resources Policy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Create Embeddings & Store Them in Chroma\n",
        "\n",
        "Use OpenAI‚Äôs embedding model (such as `text-embedding-ada-002`) to:\n",
        "\n",
        "1. üî¢ **Convert each chunk into a numerical vector**  \n",
        "   This transforms the text into a format that can be compared mathematically for similarity.\n",
        "\n",
        "2. üß† **Store the vectors in ChromaDB**  \n",
        "   This allows the system to **retrieve the most relevant chunks** when users ask questions, based on vector similarity.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LrdyzDe83v0b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3wEVyJVZ30db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Build the Q&A Pipeline\n",
        "\n",
        "Use **LangChain‚Äôs `RetrievalQA`** (or a similar approach) to create an intelligent question-answering flow.\n",
        "\n",
        "The pipeline should:\n",
        "\n",
        "1. **Take the user's question**  \n",
        "   Accept natural language input from the user (e.g., ‚ÄúWhat is Nestl√©'s policy on promotions?‚Äù)\n",
        "\n",
        "2. **Retrieve the most relevant chunks from ChromaDB**  \n",
        "   Use similarity search to find the document sections most related to the question\n",
        "\n",
        "3. **Pass those chunks to GPT-3.5 Turbo as context**  \n",
        "   Feed the retrieved text into the model so it can generate a grounded, accurate answer\n"
      ],
      "metadata": {
        "id": "ZxWDmGOl31J0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QoTQu0Cf338b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Create a Gradio Interface\n",
        "\n",
        "Use **Gradio** to allow users to type in questions and receive answers from the chatbot.\n",
        "\n",
        "Use `gr.Interface()` with the following components:\n",
        "\n",
        "- `gr.Textbox()` as the **input**  \n",
        "- `gr.Textbox()` or `gr.HTML()` as the **output**  \n",
        "- Your custom **Q&A function** as the **backend logic**\n",
        "\n",
        "This interface will let users interact with your AI assistant through a simple, user-friendly web app.\n"
      ],
      "metadata": {
        "id": "SYFry5md4N_1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGdKQXcI4Ww_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Submit\n",
        "\n",
        "Make sure your final `.ipynb` notebook meets the following requirements:\n",
        "\n",
        "- üí¨ Has **comments explaining each step** in your workflow  \n",
        "- üîÑ Shows the **full pipeline working** from document loading to answering questions  \n",
        "- üß™ Includes **example questions and answers** about Nestl√©‚Äôs HR policy\n",
        "\n",
        "Your notebook should clearly demonstrate your understanding of how to build an AI-powered Q&A assistant using OpenAI, LangChain, Chroma, and Gradio."
      ],
      "metadata": {
        "id": "8QSPQ1904XJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push to Git"
      ],
      "metadata": {
        "id": "ciIY2t4fDT3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîê Set up Git identity (only needs to be done once per Colab session)\n",
        "#!git config --global user.name \"afarabee\"\n",
        "#!git config --global user.email \"aimee.farabee@crl.com\"\n"
      ],
      "metadata": {
        "id": "7frC-AS94ecK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone #################token#############"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G8Yxp-SDq2d",
        "outputId": "8559c8ee-92d4-437e-e0e8-ef5b98ce0d90"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai_powered_hr_assistant'...\n",
            "warning: You appear to have cloned an empty repository.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move your notebook into the repo folder\n",
        "#!mv /content/AI_Powered_HR_Assistant.ipynb /content/ai_powered_hr_assistant/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gFoOAJhFUda",
        "outputId": "f838f0b2-a4ca-436f-d36c-fc4fad5d0ef5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/AI_Powered_HR_Assistant.ipynb': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls /content/*.ipynb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuQhx3sDFgqG",
        "outputId": "59af3081-8f1a-4c99-ef95-7a709786184c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/*.ipynb': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fZ55LC4-Frky"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}