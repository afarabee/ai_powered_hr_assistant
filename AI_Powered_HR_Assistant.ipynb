{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOm6KQkllBHzn8jlBbE5wrK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afarabee/ai_powered_hr_assistant/blob/main/AI_Powered_HR_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Instructions\n",
        "**Crafting an AI-Powered HR Assistant: A Use Case for Nestlé’s HR Policy Documents**\n",
        "\n",
        "---\n",
        "\n",
        "## 📝 Overview  \n",
        "The project aims to create a conversational chatbot that responds to user inquiries using PDF document information. It requires proficiency in:\n",
        "- Extracting and converting text into numerical vectors\n",
        "- Establishing an answer-finding mechanism\n",
        "- Designing a user-friendly chatbot interface with Gradio\n",
        "\n",
        "Additionally, the project emphasizes:\n",
        "- Structuring inquiries for clear communication\n",
        "- Deploying the chatbot for practical use\n",
        "- Guaranteeing the system's accessibility and efficiency in meeting user needs\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Instructions  \n",
        "- Review the learning materials and the Gradio documentation provided for the project  \n",
        "- Read the sections on **situation, task, action, and result** carefully to understand the assignment  \n",
        "- Complete and submit the assignment through the Learning Management System (LMS)  \n",
        "- Adhere closely to the provided guidelines, ensuring your submission contains all necessary analyses and interpretations  \n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Situation  \n",
        "As a developer, you have received the critical task of improving the operational efficiency of **Nestlé's Human Resources department**, a leading multinational corporation.\n",
        "\n",
        "Your toolkit includes:\n",
        "- Conversational AI technology\n",
        "- Python libraries\n",
        "- The powerful **GPT model from OpenAI**\n",
        "- The user-friendly **Gradio UI**\n",
        "\n",
        "Your mission is to integrate these advanced tools to **transform HR processes**, creating a more streamlined and efficient workflow within Nestlé.\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Task  \n",
        "Your task is to develop a **conversational chatbot** that answers queries about Nestlé's HR reports efficiently.\n",
        "\n",
        "You must:\n",
        "- Use **Python libraries**, **OpenAI's GPT model**, and **Gradio UI**\n",
        "- Create an interface that extracts and processes information from documents\n",
        "- Provide accurate responses to user queries through the chatbot\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ Action Steps  \n",
        "\n",
        "- ✅ Import essential tools and set up OpenAI's API environment  \n",
        "- ✅ Load Nestlé's HR policy using `PyPDFLoader` and split it for easy processing  \n",
        "- ✅ Create vector representations for text chunks using **ChromaDB** and **OpenAI's embeddings**  \n",
        "- ✅ Build a question-answering system using the **GPT-3.5 Turbo model** to retrieve answers  \n",
        "- ✅ Create a **prompt template** to guide the chatbot’s responses  \n",
        "- ✅ Use **Gradio** to build a user-friendly chatbot interface for interaction and information retrieval  \n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Result  \n",
        "Upon completing this project, you will submit a `.ipynb` file demonstrating your ability to use advanced AI and machine learning technologies to develop a conversational chatbot.\n",
        "\n",
        "Your submission must include:\n",
        "- Setting up the programming environment  \n",
        "- Processing text documents  \n",
        "- Creating vector representations  \n",
        "- Building a question-answering system  \n",
        "- Designing a **Gradio interface** for effective interaction  \n",
        "\n",
        "Ensure the interface is **clear, usable, and accurate** in retrieving relevant information from Nestlé’s HR policy.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "GiR0ng62svrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Requirements Breakdown\n",
        "\n",
        "You're building a smart **HR assistant** that can read the **Nestlé HR policy PDF** and answer employee questions like:\n",
        "- “What is Nestlé’s policy on promotions?”\n",
        "- “What does Nestlé offer for employee training?”\n",
        "\n",
        "This involves:\n",
        "- 🗃️ Extracting text from the PDF  \n",
        "- 🔢 Converting text chunks into numerical format (embeddings)  \n",
        "- 🧠 Storing those embeddings in a retrievable way (Chroma DB)  \n",
        "- 💬 Asking GPT to find answers using those chunks  \n",
        "- 🖼️ Displaying this interaction using Gradio  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lZreg7yi2dOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Set Up Environment\n",
        "\n",
        "## Import essential tools and set up OpenAI API's environment\n",
        "\n",
        "- Install required packages (`openai`, `langchain`, `chromadb`, `PyPDFLoader`, `gradio`, etc.)  \n",
        "- Set your OpenAI API key securely using Colab Secrets  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ev-EMDgN12U7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OInV06TArYw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eef21c8-bfbf-412a-9459-1003b6806955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m61.4/67.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.8/786.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Setup environment\n",
        "\n",
        "# 2A: Install necessary libraries\n",
        "# openai → to connect to GPT-3.5 Turbo via API\n",
        "# pypdf → to split up Nestle's HR policy for easier processing, used by LangChain's PDF loader\n",
        "# chromadb → use with OpenAI's Embeddings to create vector representations of chunks of text from the PDF that can be stored and searched for\n",
        "# gradio → to build user-friendly chatbot interface, enabling interaction and information retrieval\n",
        "# langchain → for handling document loading, splitting, and retrieval logic\n",
        "# tiktoken → used internally by OpenAI for counting tokens (needed in retrieval chains)\n",
        "# langchain_community → homes the Chroma integration\n",
        "# langchain-openai → homes the OpenAI embeddings wrapper\n",
        "\n",
        "!pip install --quiet openai pypdf chromadb gradio langchain tiktoken langchain-community langchain-openai\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "Gw5cf2COwQ6D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2B: Access my OpenAI API key securely using Colab Secrets\n",
        "# Avoids typing my key manually or exposing it in the notebook\n",
        "\n",
        "from google.colab import userdata # access my stored keys in Colab\n",
        "from openai import OpenAI         # new OpenAI client class\n",
        "\n",
        "api_key = userdata.get(\"OpenAI\")  # securely fetch my key from Secrets\n",
        "client = OpenAI(api_key=api_key)  # pass it into the OpenAI client directly\n"
      ],
      "metadata": {
        "id": "P57h2l1Tu-4c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2C: Define get_completion function using the updated SDK and secure key\n",
        "\n",
        "from google.colab import userdata               # use Colab's secure key store\n",
        "from openai import OpenAI                       # import the new OpenAI client\n",
        "\n",
        "api_key = userdata.get(\"OpenAI\")                # securely fetch my key from Secrets\n",
        "client = OpenAI(api_key=api_key)                # build a working OpenAI client with that key\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):  # send the prompt using the new SDK style\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content   # extract and return the assistant's reply\n"
      ],
      "metadata": {
        "id": "pd2J2Ca32Hhv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Step 1 E2E\n",
        "\n",
        "get_completion(\"What is the purpose of a human resources policy?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "uxvonduXwpv8",
        "outputId": "82b68004-e4a1-4563-e6f6-f0c071786c39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The purpose of a human resources policy is to provide guidelines and procedures for managing employees in a fair, consistent, and legally compliant manner. These policies help to ensure that employees are treated fairly, that their rights are protected, and that the organization operates in accordance with relevant laws and regulations. Additionally, human resources policies can help to promote a positive work environment, clarify expectations for employees, and support the overall goals and objectives of the organization.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load and Split the PDF\n",
        "\n",
        "## Load Nestle's HR Policy using PyPDFLoader and split for easy processing.\n",
        "\n",
        "Use LangChain’s PyPDFLoader to extract text from the Nestlé HR policy. Then, break the content into smaller chunks.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "N66x6P7D1tSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3A: Install addtl requiredLangChain package\n",
        "#!pip install --quiet langchain langchain-community"
      ],
      "metadata": {
        "id": "UZLD3LJn89J6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3B: Import LangChain's PDF loader and text splitter\n",
        "#from langchain.document_loaders import PyPDFLoader\n",
        "#from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "ZSkhmKPT6sGU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "local file location: file:///C:/Users/aimee/OneDrive/Documents/AppliedGenAI/Building%20LLM%20Applications/Assessment_1/1728286846_the_nestle_hr_policy_pdf_2012.pdf"
      ],
      "metadata": {
        "id": "xSH5KLLK7K-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3A: Upload HR policy PDF to Colab environment to get the correct path\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "utaSAw047PNf",
        "outputId": "14738d7d-857a-471c-a615-5a622fd2a423"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f3d73af2-da28-4329-8c3a-ed7f8157e72b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f3d73af2-da28-4329-8c3a-ed7f8157e72b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1728286846_the_nestle_hr_policy_pdf_2012.pdf to 1728286846_the_nestle_hr_policy_pdf_2012.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3B: Load the Nestlé HR policy document and define the path\n",
        "pdf_path = \"/content/1728286846_the_nestle_hr_policy_pdf_2012.pdf\" ### update path if needed ###\n",
        "\n",
        "# Use PyPDFLoader from LangChain to extract PDF content\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "\n",
        "# Step 3B: Extract the document and return a list of LangChain Document objects\n",
        "# Each page will become a separate document object (1 per PDF pg)\n",
        "pages = loader.load()\n"
      ],
      "metadata": {
        "id": "PGV0_4d76z5_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Check how many pages were loaded\n",
        "print(f\"Loaded {len(pages)} pages from the PDF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJFd-ha9omXz",
        "outputId": "891b63d5-4171-4c76-91b2-07ea36559dfc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 8 pages from the PDF.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Show characters 100-200 on page 2\n",
        "print(pages[2].page_content[100:200])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfG1JJvHxA4L",
        "outputId": "180aace9-3b34-4bc2-b5c7-37b85424cf29"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uccess and nothing can be \n",
            "achieved without their engagement. \n",
            "This document encompasses the guideli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Print the length of the first embedded document\n",
        "model_name = \"text-embedding-3-large\"\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-large\", #OpenAI embedding model to use\n",
        "    api_key=api_key                 #API key variable\n",
        ")\n",
        "text = pages[0].page_content\n",
        "embedding = embeddings.embed_query(text)\n",
        "print(len(embedding))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcXNf42vorFU",
        "outputId": "8a78fb85-6660-4c05-a22d-8be1a3e136af"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3C: Define how to chunk the text\n",
        "# I want chunks that are not too long, and that overlap slightly to preserve context\n",
        "text_splitter = RecursiveCharacterTextSplitter( #defines HOW to chunk the text\n",
        "    chunk_size=1000,      # max characters per chunk (adjustable)\n",
        "    chunk_overlap=100     # overlap to keep context from spilling over boundaries (adjustable)\n",
        ")\n",
        "\n",
        "# TEST: Text_splitter was created successfully\n",
        "print(type(text_splitter))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8OTAxM39saA",
        "outputId": "1b938139-c4e0-4ce7-db12-adf34688114d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_text_splitters.character.RecursiveCharacterTextSplitter'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3D: Split all pages into smaller chunks\n",
        "# This will give me a list of Document chunks, ready to be embedded later\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "# TEST: Preview the third chunk to see how it looks\n",
        "# Access the first element of the slice (which is the third chunk) and then its page_content\n",
        "print(chunks[2].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2FKO99s_gCV",
        "outputId": "c0276b94-6430-4ff6-8ba5-c47824410d68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Nestlé Human Resources Policy\n",
            "1\n",
            "At Nestlé, we recognize that our employees \n",
            "are the key to our success and nothing can be \n",
            "achieved without their engagement. \n",
            "This document encompasses the guidelines \n",
            "which constitute a solid basis for effective Human \n",
            "Resources Management throughout the Nestlé \n",
            "Group around the world. It explains to all Nestlé \n",
            "employees the vision and mission of the Human \n",
            "Resources function and illustrates every aspect of \n",
            "the Nestlé employee lifecycle. \n",
            "The Nestlé Management and Leadership \n",
            "Principles inspire all the Nestlé employees in their \n",
            "actions and in their dealings with others. The \n",
            "Corporate Business Principles refer to all the basic \n",
            "principles which Nestlé endorses and subscribes \n",
            "to on a worldwide basis. Both these documents \n",
            "are the pillars on which the present policy has \n",
            "been built.\n",
            "The implementation of this policy will be \n",
            "inspired by sound judgement, compliance with \n",
            "local market laws and common sense, taking into\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf2fa396",
        "outputId": "b79b9782-b613-4d24-f25c-c939bfdf3947"
      },
      "source": [
        "# TEST: Print the content of the first 5 chunks\n",
        "for i, chunk in enumerate(chunks[:3]):  #adds a counter; returns pairs of (index, value) for each item in the list; \"for\" loop iterates through the pairs\n",
        "  print(f\"--- Chunk {i+1} ---\")         #prints a header for each chunk\n",
        "  print(chunk.page_content)             #prints text content of the current chunk\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chunk 1 ---\n",
            "Policy\n",
            "Mandatory\n",
            "September  2012\n",
            "The Nestlé  \n",
            "Human Resources Policy\n",
            "--- Chunk 2 ---\n",
            "Policy\n",
            "Mandatory\n",
            "September \n",
            " 20\n",
            "12\n",
            "Issuing departement\n",
            "Hum\n",
            "an Resources\n",
            "Target audience \n",
            "All\n",
            " employees\n",
            "Approver\n",
            "Executive Board, Nestlé S.A.\n",
            "Repository\n",
            "All Nestlé Principles and Policies, Standards and  \n",
            "Guidelines can be found in the Centre online repository at:  \n",
            "http://intranet.nestle.com/nestledocs\n",
            "Copyright\n",
            " and confidentiality\n",
            "Al\n",
            "l rights belong to Nestec Ltd., Vevey, Switzerland.\n",
            "© 2012, Nestec Ltd.\n",
            "Design\n",
            "Nestec Ltd., Corporate Identity & Design,  \n",
            "Vevey, Switzerland\n",
            "Production\n",
            "brain’print GmbH, Switzerland\n",
            "Paper\n",
            "This report is printed on BVS, a paper produced  \n",
            "from well-managed forests and other controlled sources  \n",
            "certified by the Forest Stewardship Council (FSC).\n",
            "--- Chunk 3 ---\n",
            "The Nestlé Human Resources Policy\n",
            "1\n",
            "At Nestlé, we recognize that our employees \n",
            "are the key to our success and nothing can be \n",
            "achieved without their engagement. \n",
            "This document encompasses the guidelines \n",
            "which constitute a solid basis for effective Human \n",
            "Resources Management throughout the Nestlé \n",
            "Group around the world. It explains to all Nestlé \n",
            "employees the vision and mission of the Human \n",
            "Resources function and illustrates every aspect of \n",
            "the Nestlé employee lifecycle. \n",
            "The Nestlé Management and Leadership \n",
            "Principles inspire all the Nestlé employees in their \n",
            "actions and in their dealings with others. The \n",
            "Corporate Business Principles refer to all the basic \n",
            "principles which Nestlé endorses and subscribes \n",
            "to on a worldwide basis. Both these documents \n",
            "are the pillars on which the present policy has \n",
            "been built.\n",
            "The implementation of this policy will be \n",
            "inspired by sound judgement, compliance with \n",
            "local market laws and common sense, taking into\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Create Embeddings & Store Them in Chroma\n",
        "\n",
        "##Create vector representations for text chunks using Chroma dB and OpenAI's embeddings.\n",
        "\n",
        "Use OpenAI’s embedding model (such as `text-embedding-3-large`) to:\n",
        "\n",
        "1. 🔢 **Convert each chunk into a numerical vector**  \n",
        "   This transforms the text into a format that can be compared mathematically for similarity.\n",
        "\n",
        "2. 🧠 **Store the vectors in ChromaDB**  \n",
        "   This allows the system to **retrieve the most relevant chunks** when users ask questions, based on vector similarity.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LrdyzDe83v0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4a - Store my Chroma DB in GDrive so it persists across sessions\n",
        "\n",
        "Created the folder\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs(persist_directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "vZy6-CFNvWqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount my Google Drive so Colab can read/write to it.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os # Import the os module\n",
        "\n",
        "# tells the rest of the code where to save/load the DB\n",
        "persist_directory = \"/content/drive/MyDrive/ai_powered_hr_assistant/chroma_nestle\"\n",
        "\n",
        "# Validate that the path is ready.\n",
        "print(\"Chroma DB folder set to:\", persist_directory)\n",
        "print(\"Folder exists?\", os.path.isdir(persist_directory))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qQUCv_WvIOc",
        "outputId": "175f5861-d421-46c6-af49-004e03aef1c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Chroma DB folder set to: /content/drive/MyDrive/ai_powered_hr_assistant/chroma_nestle\n",
            "Folder exists? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4b: Build the embeddings object"
      ],
      "metadata": {
        "id": "BkQdcJA20cpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2TWPTg5bxi5",
        "outputId": "dda6a09b-17c2-4f2f-a48f-ba85bf731c6b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-chroma\n",
            "  Downloading langchain_chroma-0.2.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: langchain-core>=0.3.70 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma) (0.3.74)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma) (2.0.2)\n",
            "Requirement already satisfied: chromadb>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma) (1.0.16)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.14.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.21.4)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (3.11.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.25.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain-chroma) (0.4.12)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain-chroma) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain-chroma) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.70->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.10)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain-chroma) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain-chroma) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.9->langchain-chroma) (0.57b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (0.34.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (1.1.7)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.6.1)\n",
            "Downloading langchain_chroma-0.2.5-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: langchain-chroma\n",
            "Successfully installed langchain-chroma-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma"
      ],
      "metadata": {
        "id": "8XfElFl-b01C"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed + persist (OpenAI’s current recs: text-embedding-3-small/large)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=api_key) #load API key again if needed\n"
      ],
      "metadata": {
        "id": "1DMZfs5Ovyx8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the embeddings model directly during Chroma initialization\n",
        "# Docs are auto persisted in new versions of Chroma, no need for vectordb.persist()\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "5JEuvjp-38Wr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the vector store into a Retriever object\n",
        "# The Chroma vector store object (vectordb) has a method to easily convert it into a retriever\n",
        "\n",
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "oP9_K3yXr_ws"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "246d8084",
        "outputId": "dd821d82-8562-4f8b-e11a-ae78940349fb"
      },
      "source": [
        "# Test the vector database by performing a similarity search\n",
        "query = \"What is Nestlé's policy on employee training?\"\n",
        "# Use k= to retrieve only the top n most relevant documents\n",
        "docs = vectordb.similarity_search(query, k=3)\n",
        "\n",
        "# Print the content of the retrieved documents to see if they are relevant\n",
        "print(f\"Retrieved {len(docs)} documents:\")\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"--- Retrieved Document {i+1} ---\")\n",
        "    print(doc.page_content)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 3 documents:\n",
            "--- Retrieved Document 1 ---\n",
            "The Nestlé Human Resources Policy\n",
            "4\n",
            "Learning is part of the Company culture.\n",
            "Employees at all levels are systematically \n",
            "encouraged to consider how they upgrade their \n",
            "knowledge and skills.\n",
            "The Company determines training and deve-\n",
            "lopment priorities. The responsibility for turning \n",
            "these into actions is shared between employees, \n",
            "line managers and the Human Resources. \n",
            "Experience and on-the-job training are the \n",
            "primary source of learning. Managers are \n",
            "responsible for guiding and coaching employees \n",
            "to succeed in their current positions.  \n",
            "Nestlé employees understand the importance \n",
            "of continuous improvement, as well as sharing \n",
            "knowledge and ideas freely with others. Practices \n",
            "such as lateral professional development, \n",
            "extension of responsibilities, and cross functional \n",
            "teams are encouraged to acquire additional skills, \n",
            "enrich job content and widen accountability.\n",
            "Nestlé also offers a comprehensive range of \n",
            "training activities and methodologies to support\n",
            "\n",
            "\n",
            "--- Retrieved Document 2 ---\n",
            "The Nestlé Human Resources Policy\n",
            "4\n",
            "Learning is part of the Company culture.\n",
            "Employees at all levels are systematically \n",
            "encouraged to consider how they upgrade their \n",
            "knowledge and skills.\n",
            "The Company determines training and deve-\n",
            "lopment priorities. The responsibility for turning \n",
            "these into actions is shared between employees, \n",
            "line managers and the Human Resources. \n",
            "Experience and on-the-job training are the \n",
            "primary source of learning. Managers are \n",
            "responsible for guiding and coaching employees \n",
            "to succeed in their current positions.  \n",
            "Nestlé employees understand the importance \n",
            "of continuous improvement, as well as sharing \n",
            "knowledge and ideas freely with others. Practices \n",
            "such as lateral professional development, \n",
            "extension of responsibilities, and cross functional \n",
            "teams are encouraged to acquire additional skills, \n",
            "enrich job content and widen accountability.\n",
            "Nestlé also offers a comprehensive range of \n",
            "training activities and methodologies to support\n",
            "\n",
            "\n",
            "--- Retrieved Document 3 ---\n",
            "The Nestlé Human Resources Policy\n",
            "4\n",
            "Learning is part of the Company culture.\n",
            "Employees at all levels are systematically \n",
            "encouraged to consider how they upgrade their \n",
            "knowledge and skills.\n",
            "The Company determines training and deve-\n",
            "lopment priorities. The responsibility for turning \n",
            "these into actions is shared between employees, \n",
            "line managers and the Human Resources. \n",
            "Experience and on-the-job training are the \n",
            "primary source of learning. Managers are \n",
            "responsible for guiding and coaching employees \n",
            "to succeed in their current positions.  \n",
            "Nestlé employees understand the importance \n",
            "of continuous improvement, as well as sharing \n",
            "knowledge and ideas freely with others. Practices \n",
            "such as lateral professional development, \n",
            "extension of responsibilities, and cross functional \n",
            "teams are encouraged to acquire additional skills, \n",
            "enrich job content and widen accountability.\n",
            "Nestlé also offers a comprehensive range of \n",
            "training activities and methodologies to support\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Build the Q&A Pipeline\n",
        "\n",
        "##*Build a question-answering system using the GPT-3.5 Turbo model to retrieve answers from text chunks.*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Build a RetrievalQA chain that uses:\n",
        " - my prompt template (QA_CHAIN_PROMPT)\n",
        " - my retriever (built from the Chroma vectorstore)\n",
        " - an LLM (ChatOpenAI) with temperature=0 for consistent answers\n",
        "\n",
        "----\n",
        "Use **LangChain’s `RetrievalQA`** (or a similar approach) to create an intelligent question-answering flow.\n",
        "\n",
        "The pipeline should:\n",
        "\n",
        "1. **Take the user's question**  \n",
        "   Accept natural language input from the user (e.g., “What is Nestlé's policy on promotions?”)\n",
        "\n",
        "2. **Retrieve the most relevant chunks from ChromaDB**  \n",
        "   Use similarity search to find the document sections most related to the question\n",
        "\n",
        "3. **Pass those chunks to GPT-3.5 Turbo as context**  \n",
        "   Feed the retrieved text into the model so it can generate a grounded, accurate answer"
      ],
      "metadata": {
        "id": "ZxWDmGOl31J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5.1: Import the LangChain OpenAI chat wrapper\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "VZqgBcVv4RoW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5.2: Initialize the language model (GPT-3.5 Turbo); Create the LLM object\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    api_key=api_key\n",
        ")"
      ],
      "metadata": {
        "id": "2w-Ra69ip1pK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa89bcbe",
        "outputId": "77e6b1f1-25e5-45c5-a74b-73611775373c"
      },
      "source": [
        "# TEST the LLM connectivity\n",
        "print(llm.invoke(\"Hello, how are you?\"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with anything you need. How can I assist you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 13, 'total_tokens': 50, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C3qlILtL7uiL91iIxBHSkyk2ukCo9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--d275066f-3693-44db-8add-8ca3eb8114b6-0' usage_metadata={'input_tokens': 13, 'output_tokens': 37, 'total_tokens': 50, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e62a7eec"
      },
      "source": [
        "#Step 5.3: Assemble the retriever\n",
        "from langchain.chains import RetrievalQA\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Retrieve relevant documents from the vectordb using similarity search with RetrievalQA\n",
        "\n",
        "query = \"What ensures the success of Nestle as a company?\"\n",
        "docs = vectordb.similarity_search(query)\n",
        "\n",
        "# Retrieve only the top n most relevant documents\n",
        "docs = vectordb.similarity_search(query, k=3)\n",
        "\n",
        "# Print the content of the retrieved documents to see if they are relevant\n",
        "print(f\"Retrieved {len(docs)} documents:\")\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"--- Retrieved Document {i+1} ---\")\n",
        "    print(doc.page_content)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "QoTQu0Cf338b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6648d51-a7e6-4e4e-c77d-da15347a9494"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 3 documents:\n",
            "--- Retrieved Document 1 ---\n",
            "of a global company with the creativity and \n",
            "knowledge of a local business. As a result, people \n",
            "can have far-reaching influence every day and \n",
            "explore their full long-term potential, propelled by \n",
            "continual support and a collaborative approach by \n",
            "line managers and employees.\n",
            "Corporate policy: \n",
            "Nestlé on the Move\n",
            " A flexible and dynamic organisation\n",
            "\n",
            "\n",
            "--- Retrieved Document 2 ---\n",
            "of a global company with the creativity and \n",
            "knowledge of a local business. As a result, people \n",
            "can have far-reaching influence every day and \n",
            "explore their full long-term potential, propelled by \n",
            "continual support and a collaborative approach by \n",
            "line managers and employees.\n",
            "Corporate policy: \n",
            "Nestlé on the Move\n",
            " A flexible and dynamic organisation\n",
            "\n",
            "\n",
            "--- Retrieved Document 3 ---\n",
            "of a global company with the creativity and \n",
            "knowledge of a local business. As a result, people \n",
            "can have far-reaching influence every day and \n",
            "explore their full long-term potential, propelled by \n",
            "continual support and a collaborative approach by \n",
            "line managers and employees.\n",
            "Corporate policy: \n",
            "Nestlé on the Move\n",
            " A flexible and dynamic organisation\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5.4: Create a Prompt Template\n",
        "\n",
        "# prompt_template: tell it what to use (the context), what not to do, and limit output length\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "zkmr4GqSp-MQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###Breakdown of Step 5.4 \"Create a Prompt Template\"\n",
        "\n",
        "####1. The instructions to the LLM:\n",
        "```\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum.\n",
        "\n",
        "```\n",
        "These are system-like instructions baked into the prompt to guide how the LLM should behave.\n",
        "\n",
        "They:\n",
        "\n",
        "Tell it what to use (the context we’ll give it)\n",
        "\n",
        "Tell it what not to do (no guessing / hallucinating)\n",
        "\n",
        "Put a limit on output length (max three sentences)\n",
        "\n",
        "\n",
        "####2. {context} placeholder\n",
        "\n",
        "```\n",
        "{context}\n",
        "\n",
        "```\n",
        "This is a placeholder in a Python format string.\n",
        "\n",
        "Later, you’ll insert the actual retrieved text chunks here:\n",
        "\n",
        "\n",
        "```\n",
        "prompt = prompt_template.format(context=\"policy text...\", question=\"What is Nestlé's policy on training?\")\n",
        "\n",
        "```\n",
        "The curly braces {} get replaced with real values by .format().\n",
        "\n",
        "####3. The question and answer section\n",
        "\n",
        "\n",
        "```\n",
        "Question: {question}\n",
        "Answer:\n",
        "\n",
        "```\n",
        "{question} is another placeholder where you’ll insert the actual user’s question.\n",
        "\n",
        "The Answer: line is left blank so the LLM will write the answer directly after it.\n",
        "\n",
        "\n",
        "####4. Putting it together\n",
        "When you run:\n",
        "```\n",
        "    prompt = prompt_template.format(\n",
        "        context=\"Nestlé provides a wide range of training programs...\",\n",
        "        question=\"What is Nestlé's policy on training?\"\n",
        "    )\n",
        "    print(prompt)\n",
        "```\n",
        "\n",
        "You get:\n",
        "```\n",
        "    Use the following pieces of context to answer the question at the end.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    Use three sentences maximum.\n",
        "\n",
        "    Nestlé provides a wide range of training programs...\n",
        "\n",
        "    Question: What is Nestlé's policy on training?\n",
        "    Answer:\n",
        "```\n",
        "\n",
        "You can reuse the same template for every question.\n",
        "\n",
        "The {context} gets filled dynamically with retrieved chunks from Chroma.\n",
        "\n",
        "The {question} gets filled with the user’s query.\n",
        "\n",
        "This keeps your LLM grounded in the right document and avoids hallucination.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mseZCDSf7XAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5.5: Turn my text template into a PromptTemplate object that L.C. can use\n",
        "# L.C. will dynamically construct the final prompt sent to GPT-3.5 Turbo during the Q&A process.\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template) #takes raw string input and converts into PromptTemplate that L.C. can work with\n",
        "\n",
        "# This method parses the string, identifies the placeholders, & sets up the structure so when P.T. is used later, L.C. knows where to inject the actual document context and users' question.\n",
        "# Required b/c L.C. chains, like RetrievalQA, are designed to work w/ structured objects"
      ],
      "metadata": {
        "id": "W_PEHMDXwO-7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5.6: Create the RetrievalQA chain & assign it to the qa_chain variable\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    # initialized model object = GPT-3.5 Turbo with temperature=0\n",
        "    llm=llm,\n",
        "    # passes retriever object (created from vectordb); tells the chain where to get the relevant document chunks from when a query comes in\n",
        "    retriever=retriever,\n",
        "    # Optional: returns the chunks used to generate the answer\n",
        "    return_source_documents=True,\n",
        "    # RetrievalQA uses chain type=\"stuff,\" by default. This part tells the \"stuff\" chain to use the custom QA_CHAIN_PROMPT as the template for the prompt it constructs.\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT} # Pass the prompt template to the chain. allows you to pass additional parameters to the underlying chain type being used by RetrievalQA\n",
        ")"
      ],
      "metadata": {
        "id": "Fb8JmkidwRwu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST the chain by asking a question\n",
        "query = \"What ensures the success of Nestle as a company?\"\n",
        "response = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "# Print the Question and Answer\n",
        "print(\"Question:\", query)\n",
        "print(\"Answer:\", response['result'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo9Tq1imwVIg",
        "outputId": "e9802b12-65ec-41be-c99a-0239d1c8970b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What ensures the success of Nestle as a company?\n",
            "Answer: Nestle's success is ensured by combining the resources of a global company with the creativity and knowledge of a local business, allowing people to have far-reaching influence and explore their full potential. This is further supported by continual support and a collaborative approach by line managers and employees, as outlined in the corporate policy \"Nestlé on the Move.\" The company's flexible and dynamic organizational structure also contributes to its success.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Print source documents\n",
        "print(\"\\nSource Documents:\")\n",
        "for doc in response['source_documents']:\n",
        "     print(doc.page_content[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfh7txbx2QW9",
        "outputId": "597b5788-0a01-4c08-bb71-62a351e5e733"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Source Documents:\n",
            "of a global company with the creativity and \n",
            "knowledge of a local business. As a result, people \n",
            "can have far-reaching influence every day and \n",
            "explore their full long-term potential, propelled by \n",
            "co\n",
            "of a global company with the creativity and \n",
            "knowledge of a local business. As a result, people \n",
            "can have far-reaching influence every day and \n",
            "explore their full long-term potential, propelled by \n",
            "co\n",
            "of a global company with the creativity and \n",
            "knowledge of a local business. As a result, people \n",
            "can have far-reaching influence every day and \n",
            "explore their full long-term potential, propelled by \n",
            "co\n",
            "of a global company with the creativity and \n",
            "knowledge of a local business. As a result, people \n",
            "can have far-reaching influence every day and \n",
            "explore their full long-term potential, propelled by \n",
            "co\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST with a different question\n",
        "query = \"What are “Nestlé Management and Leadership Principles?\"\n",
        "response = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "# Print the Question and Answer\n",
        "print(\"Question:\", query)\n",
        "print(\"Answer:\", response['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj3EizZo2p-n",
        "outputId": "78716235-3d61-48a6-89b3-42afccb90622"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are “Nestlé Management and Leadership Principles?\n",
            "Answer: The Nestlé Management and Leadership Principles inspire all Nestlé employees in their actions and dealings with others. These principles are part of the pillars on which the present policy has been built. They guide employees on how to conduct themselves in their roles within the company.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Create a Gradio Interface\n",
        "\n",
        "## *Use Gradio to build a user-friendly chatbot interface, enabling interaction and information retrieval.*\n",
        "\n",
        "Allow users to type in questions and receive answers from the chatbot.\n",
        "\n"
      ],
      "metadata": {
        "id": "SYFry5md4N_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6.1: Install gradio (safe to re-run; does nothing if it's already there)\n",
        "!pip install --quiet gradio"
      ],
      "metadata": {
        "id": "dGdKQXcI4Ww_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6.2: Import the library to build the web UI\n",
        "import gradio as gr\n",
        "\n",
        "def greet(name, intensity):\n",
        "    return \"Hello \" * intensity + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=greet,\n",
        "    inputs=[\"text\", \"slider\"],\n",
        "    outputs=[\"text\"],\n",
        ")\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "eIhfMJyFEclq",
        "outputId": "b6b909b6-feda-4385-c8c7-03fe49117832"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://dda0eaf5e44ce53f29.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dda0eaf5e44ce53f29.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6.3: Print the version to confirm it's available in this runtime\n",
        "print(\"Gradio version:\", gr.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbIzdXW_EkMl",
        "outputId": "df4f7e8f-fdcc-4049-c5f0-81d42cbba947"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio version: 5.41.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6.4: Backend function that takes the user's question (string), sends to RetrievalQA (qa_chain) chain and returns an answer (string)\n",
        "def answer_question(user_question: str) -> str:\n",
        "    \"\"\"\n",
        "    Backend:\n",
        "    - Accept a question string from the UI\n",
        "    - Use the RetrievalQA chain (qa_chain) to get an answer\n",
        "    - Return a clean text response for display in Gradio\n",
        "    \"\"\"\n",
        "\n",
        "    # TEST: make sure qa_chain exists (built in Step 5)\n",
        "    assert 'qa_chain' in globals(), \"qa_chain not found. Build the RetrievalQA chain before launching Gradio.\"\n",
        "\n",
        "\n",
        "    # Handle Empty input\n",
        "    if not user_question or not user_question.strip():\n",
        "        return \"Please enter a question about the Nestlé HR policy.\"\n",
        "\n",
        "\n",
        "    # Process the question using the QA chain\n",
        "    # Assuming qa_chain is defined globally or passed in\n",
        "    response = qa_chain.invoke({\"query\": user_question})\n",
        "\n",
        "\n",
        "    # Extract the answer text from the response, with default message if key isn't found or is None\n",
        "    answer_text = response.get('result', 'Sorry, I could not find an answer. Would you like to try a different question?')\n",
        "\n",
        "\n",
        "    # Return final message to the UI\n",
        "    return answer_text"
      ],
      "metadata": {
        "id": "TDZVypx_Etki"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: call the backend directly (bypassing Gradio) to verify it returns an answer\n",
        "test_q = \"What does Nestlé say about training and development?\"\n",
        "\n",
        "# Call the function to print the answer\n",
        "print(answer_question(test_q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJsd7fi4FrYR",
        "outputId": "14cf7ab3-f7cc-4f9d-fbaa-d37d0cb24527"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nestlé emphasizes the importance of continuous learning and upgrading knowledge and skills for employees at all levels. The company determines training and development priorities and encourages employees, line managers, and HR to collaborate in implementing them. Nestlé promotes on-the-job training, coaching, and sharing knowledge to support professional development and growth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio UI to call the backend function and launch the Gradio app\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# Create input + output widgets\n",
        "#   Small text box to type a question\n",
        "#   Larger text box to display answer\n",
        "question_box = gr.Textbox(\n",
        "    label=\"Ask a question about the Nestlé HR policy\",\n",
        "    placeholder=\"e.g., What does Nestlé say about training and development?\",\n",
        "    lines=2\n",
        ")\n",
        "answer_box = gr.Textbox(\n",
        "    label=\"Answer\",\n",
        "    lines=10\n",
        ")\n",
        "\n",
        "# Wrap backend function + widgets into a simple interface\n",
        "demo = gr.Interface(\n",
        "    fn=answer_question,     # function defined in step 6.4\n",
        "    inputs=question_box,    # input component(s)\n",
        "    outputs=answer_box,     # output component(s)\n",
        "    title=\"Nestlé HR Assistant\",\n",
        "    description=\"Type your question below and I’ll do my best to help.\"\n",
        ")\n",
        "\n",
        "# Launch the app (share=True gives me a temporary public link I can submit/test)\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "Ibe2z4qWKkpu",
        "outputId": "b2014dbe-21ed-48eb-9019-cd47e4fa41da"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://122d7f888d57f2225d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://122d7f888d57f2225d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Submit\n",
        "\n",
        "Make sure your final `.ipynb` notebook meets the following requirements:\n",
        "\n",
        "- 💬 Has **comments explaining each step** in your workflow  \n",
        "- 🔄 Shows the **full pipeline working** from document loading to answering questions  \n",
        "- 🧪 Includes **example questions and answers** about Nestlé’s HR policy\n",
        "\n",
        "Your notebook should clearly demonstrate your understanding of how to build an AI-powered Q&A assistant using OpenAI, LangChain, Chroma, and Gradio."
      ],
      "metadata": {
        "id": "8QSPQ1904XJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push to Git"
      ],
      "metadata": {
        "id": "ciIY2t4fDT3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔐 Set up Git identity (only needs to be done once per Colab session)\n",
        "#!git config --global user.name \"afarabee\"\n",
        "#!git config --global user.email \"aimee.farabee@crl.com\"\n"
      ],
      "metadata": {
        "id": "7frC-AS94ecK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone #################token#############"
      ],
      "metadata": {
        "id": "5G8Yxp-SDq2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move your notebook into the repo folder\n",
        "#!mv /content/AI_Powered_HR_Assistant.ipynb /content/ai_powered_hr_assistant/\n"
      ],
      "metadata": {
        "id": "3gFoOAJhFUda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls /content/*.ipynb\n"
      ],
      "metadata": {
        "id": "tuQhx3sDFgqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fZ55LC4-Frky"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}