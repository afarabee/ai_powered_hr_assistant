{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNE3xVPpKRyLZw3iJEOsqHL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afarabee/ai_powered_hr_assistant/blob/main/AI_Powered_HR_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Instructions\n",
        "**Crafting an AI-Powered HR Assistant: A Use Case for Nestlé’s HR Policy Documents**\n",
        "\n",
        "---\n",
        "\n",
        "## 📝 Overview  \n",
        "The project aims to create a conversational chatbot that responds to user inquiries using PDF document information. It requires proficiency in:\n",
        "- Extracting and converting text into numerical vectors\n",
        "- Establishing an answer-finding mechanism\n",
        "- Designing a user-friendly chatbot interface with Gradio\n",
        "\n",
        "Additionally, the project emphasizes:\n",
        "- Structuring inquiries for clear communication\n",
        "- Deploying the chatbot for practical use\n",
        "- Guaranteeing the system's accessibility and efficiency in meeting user needs\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Instructions  \n",
        "- Review the learning materials and the Gradio documentation provided for the project  \n",
        "- Read the sections on **situation, task, action, and result** carefully to understand the assignment  \n",
        "- Complete and submit the assignment through the Learning Management System (LMS)  \n",
        "- Adhere closely to the provided guidelines, ensuring your submission contains all necessary analyses and interpretations  \n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Situation  \n",
        "As a developer, you have received the critical task of improving the operational efficiency of **Nestlé's Human Resources department**, a leading multinational corporation.\n",
        "\n",
        "Your toolkit includes:\n",
        "- Conversational AI technology\n",
        "- Python libraries\n",
        "- The powerful **GPT model from OpenAI**\n",
        "- The user-friendly **Gradio UI**\n",
        "\n",
        "Your mission is to integrate these advanced tools to **transform HR processes**, creating a more streamlined and efficient workflow within Nestlé.\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Task  \n",
        "Your task is to develop a **conversational chatbot** that answers queries about Nestlé's HR reports efficiently.\n",
        "\n",
        "You must:\n",
        "- Use **Python libraries**, **OpenAI's GPT model**, and **Gradio UI**\n",
        "- Create an interface that extracts and processes information from documents\n",
        "- Provide accurate responses to user queries through the chatbot\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ Action Steps  \n",
        "\n",
        "- ✅ Import essential tools and set up OpenAI's API environment  \n",
        "- ✅ Load Nestlé's HR policy using `PyPDFLoader` and split it for easy processing  \n",
        "- ✅ Create vector representations for text chunks using **ChromaDB** and **OpenAI's embeddings**  \n",
        "- ✅ Build a question-answering system using the **GPT-3.5 Turbo model** to retrieve answers  \n",
        "- ✅ Create a **prompt template** to guide the chatbot’s responses  \n",
        "- ✅ Use **Gradio** to build a user-friendly chatbot interface for interaction and information retrieval  \n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Result  \n",
        "Upon completing this project, you will submit a `.ipynb` file demonstrating your ability to use advanced AI and machine learning technologies to develop a conversational chatbot.\n",
        "\n",
        "Your submission must include:\n",
        "- Setting up the programming environment  \n",
        "- Processing text documents  \n",
        "- Creating vector representations  \n",
        "- Building a question-answering system  \n",
        "- Designing a **Gradio interface** for effective interaction  \n",
        "\n",
        "Ensure the interface is **clear, usable, and accurate** in retrieving relevant information from Nestlé’s HR policy.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "GiR0ng62svrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Requirements Breakdown\n",
        "\n",
        "You're building a smart **HR assistant** that can read the **Nestlé HR policy PDF** and answer employee questions like:\n",
        "- “What is Nestlé’s policy on promotions?”\n",
        "- “What does Nestlé offer for employee training?”\n",
        "\n",
        "This involves:\n",
        "- 🗃️ Extracting text from the PDF  \n",
        "- 🔢 Converting text chunks into numerical format (embeddings)  \n",
        "- 🧠 Storing those embeddings in a retrievable way (Chroma DB)  \n",
        "- 💬 Asking GPT to find answers using those chunks  \n",
        "- 🖼️ Displaying this interaction using Gradio  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lZreg7yi2dOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Set Up Environment\n",
        "- Install required packages (`openai`, `langchain`, `chromadb`, `PyPDFLoader`, `gradio`, etc.)  \n",
        "- Set your OpenAI API key securely using Colab Secrets  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ev-EMDgN12U7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OInV06TArYw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3aeb617-a265-4fdf-d5a3-c26d8928a259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Step 2: Setup environment\n",
        "\n",
        "# 2A: Install necessary libraries\n",
        "# openai → to connect to GPT-3.5 Turbo via API\n",
        "# pypdf → to split up Nestle's HR policy for easier processing, used by LangChain's PDF loader\n",
        "# chromadb → use with OpenAI's Embeddings to create vector representations of chunks of text from the PDF that can be stored and searched for\n",
        "# gradio → to build user-friendly chatbot interface, enabling interaction and information retrieval\n",
        "# langchain → for handling document loading, splitting, and retrieval logic\n",
        "# tiktoken → used internally by OpenAI for counting tokens (needed in retrieval chains)\n",
        "# langchain_community → homes the Chroma integration\n",
        "# langchain-openai → homes the OpenAI embeddings wrapper\n",
        "\n",
        "!pip install --quiet openai pypdf chromadb gradio langchain tiktoken langchain-community langchain-openai\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "Gw5cf2COwQ6D"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2B: Access my OpenAI API key securely using Colab Secrets\n",
        "# Avoids typing my key manually or exposing it in the notebook\n",
        "\n",
        "from google.colab import userdata # access my stored keys in Colab\n",
        "from openai import OpenAI         # new OpenAI client class\n",
        "\n",
        "api_key = userdata.get(\"OpenAI\")  # securely fetch my key from Secrets\n",
        "client = OpenAI(api_key=api_key)  # pass it into the OpenAI client directly\n"
      ],
      "metadata": {
        "id": "P57h2l1Tu-4c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2C: Define get_completion function using the updated SDK and secure key\n",
        "\n",
        "from google.colab import userdata               # use Colab's secure key store\n",
        "from openai import OpenAI                       # import the new OpenAI client\n",
        "\n",
        "api_key = userdata.get(\"OpenAI\")                # securely fetch my key from Secrets\n",
        "client = OpenAI(api_key=api_key)                # build a working OpenAI client with that key\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):  # send the prompt using the new SDK style\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content   # extract and return the assistant's reply\n"
      ],
      "metadata": {
        "id": "pd2J2Ca32Hhv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Step 1 E2E\n",
        "\n",
        "get_completion(\"What is the purpose of a human resources policy?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "uxvonduXwpv8",
        "outputId": "97919f7a-cbfc-443a-d7cc-2863da986fd3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The purpose of a human resources policy is to provide guidelines and procedures for managing employees in a fair, consistent, and legally compliant manner. These policies help to ensure that employees are treated fairly, that their rights are protected, and that the organization operates in accordance with relevant laws and regulations. Additionally, human resources policies can help to promote a positive work environment, clarify expectations for employees, and support the overall goals and objectives of the organization.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load and Split the PDF\n",
        "Use LangChain’s PyPDFLoader to extract text from the Nestlé HR policy. Then, break the content into smaller chunks.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "N66x6P7D1tSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3A: Install addtl requiredLangChain package\n",
        "#!pip install --quiet langchain langchain-community"
      ],
      "metadata": {
        "id": "UZLD3LJn89J6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "524ba9f9-25be-4451-fb97-168a56218d57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3B: Import LangChain's PDF loader and text splitter\n",
        "#from langchain.document_loaders import PyPDFLoader\n",
        "#from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "ZSkhmKPT6sGU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "local file location: file:///C:/Users/aimee/OneDrive/Documents/AppliedGenAI/Building%20LLM%20Applications/Assessment_1/1728286846_the_nestle_hr_policy_pdf_2012.pdf"
      ],
      "metadata": {
        "id": "xSH5KLLK7K-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3A: Upload HR policy PDF to Colab environment to get the correct path\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "utaSAw047PNf",
        "outputId": "cca1c414-6dbd-4f5d-d39c-141c5cb837d8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a94da829-06a6-4c4a-8706-464db897c578\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a94da829-06a6-4c4a-8706-464db897c578\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1728286846_the_nestle_hr_policy_pdf_2012.pdf to 1728286846_the_nestle_hr_policy_pdf_2012 (2).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3B: Load the Nestlé HR policy document and define the path\n",
        "pdf_path = \"/content/1728286846_the_nestle_hr_policy_pdf_2012 (2).pdf\" ### update path if needed ###\n",
        "\n",
        "# Use PyPDFLoader from LangChain to extract PDF content\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "\n",
        "# Step 3B: Extract the document and return a list of LangChain Document objects\n",
        "# Each page will become a separate document object (1 per PDF pg)\n",
        "pages = loader.load()\n",
        "\n",
        "# Test: Check how many pages were loaded\n",
        "print(f\"Loaded {len(pages)} pages from the PDF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGV0_4d76z5_",
        "outputId": "a9ac7b01-db51-4fd2-d7d6-bed29e132819"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 8 pages from the PDF.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test: Show characters 100-200 on page 2\n",
        "print(pages[2].page_content[100:200])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfG1JJvHxA4L",
        "outputId": "52846fbb-eade-4117-cbe8-4c5c1b5cc7f7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uccess and nothing can be \n",
            "achieved without their engagement. \n",
            "This document encompasses the guideli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3C: Define how to chunk the text\n",
        "# I want chunks that are not too long, and that overlap slightly to preserve context\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # max characters per chunk (adjustable)\n",
        "    chunk_overlap=100     # overlap to keep context from spilling over boundaries\n",
        ")\n",
        "\n",
        "# Test: Text_splitter was created successfully\n",
        "print(type(text_splitter))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8OTAxM39saA",
        "outputId": "cba87cdc-7dde-4009-ee57-6469a94a47bf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_text_splitters.character.RecursiveCharacterTextSplitter'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3D: Split all pages into smaller chunks\n",
        "# This will give me a list of Document chunks, ready to be embedded later\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "# Test: Preview the third chunk to see how it looks\n",
        "print(chunks[2].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2FKO99s_gCV",
        "outputId": "4e7924a7-c335-4cd0-d53d-8623b7b09616"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Nestlé Human Resources Policy\n",
            "1\n",
            "At Nestlé, we recognize that our employees \n",
            "are the key to our success and nothing can be \n",
            "achieved without their engagement. \n",
            "This document encompasses the guidelines \n",
            "which constitute a solid basis for effective Human \n",
            "Resources Management throughout the Nestlé \n",
            "Group around the world. It explains to all Nestlé \n",
            "employees the vision and mission of the Human \n",
            "Resources function and illustrates every aspect of \n",
            "the Nestlé employee lifecycle. \n",
            "The Nestlé Management and Leadership \n",
            "Principles inspire all the Nestlé employees in their \n",
            "actions and in their dealings with others. The \n",
            "Corporate Business Principles refer to all the basic \n",
            "principles which Nestlé endorses and subscribes \n",
            "to on a worldwide basis. Both these documents \n",
            "are the pillars on which the present policy has \n",
            "been built.\n",
            "The implementation of this policy will be \n",
            "inspired by sound judgement, compliance with \n",
            "local market laws and common sense, taking into\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Create Embeddings & Store Them in Chroma\n",
        "\n",
        "Use OpenAI’s embedding model (such as `text-embedding-3-large`) to:\n",
        "\n",
        "1. 🔢 **Convert each chunk into a numerical vector**  \n",
        "   This transforms the text into a format that can be compared mathematically for similarity.\n",
        "\n",
        "2. 🧠 **Store the vectors in ChromaDB**  \n",
        "   This allows the system to **retrieve the most relevant chunks** when users ask questions, based on vector similarity.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LrdyzDe83v0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4a - Store my Chroma DB in GDrive so it persists across sessions\n",
        "\n",
        "Created the folder\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs(persist_directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "vZy6-CFNvWqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount my Google Drive so Colab can read/write to it.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# tells the rest of the code where to save/load the DB\n",
        "persist_directory = \"/content/drive/MyDrive/ai_powered_hr_assistant/chroma_nestle\"\n",
        "\n",
        "# Validate that the path is ready.\n",
        "print(\"Chroma DB folder set to:\", persist_directory)\n",
        "print(\"Folder exists?\", os.path.isdir(persist_directory))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qQUCv_WvIOc",
        "outputId": "bb5cab43-1529-48ec-9956-d346db0ae49c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Chroma DB folder set to: /content/drive/MyDrive/ai_powered_hr_assistant/chroma_nestle\n",
            "Folder exists? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4b: Build the embeddings object"
      ],
      "metadata": {
        "id": "BkQdcJA20cpt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sEQC3s9Lv0iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed + persist (OpenAI’s current recs: text-embedding-3-small/large)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "vectordb = Chroma(collection_name=\"sops\", embedding_function=embeddings, persist_directory=\"./chroma\")\n",
        "vectordb.add_documents(chunks)\n",
        "vectordb.persist()"
      ],
      "metadata": {
        "id": "1DMZfs5Ovyx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Build the Q&A Pipeline\n",
        "\n",
        "Use **LangChain’s `RetrievalQA`** (or a similar approach) to create an intelligent question-answering flow.\n",
        "\n",
        "The pipeline should:\n",
        "\n",
        "1. **Take the user's question**  \n",
        "   Accept natural language input from the user (e.g., “What is Nestlé's policy on promotions?”)\n",
        "\n",
        "2. **Retrieve the most relevant chunks from ChromaDB**  \n",
        "   Use similarity search to find the document sections most related to the question\n",
        "\n",
        "3. **Pass those chunks to GPT-3.5 Turbo as context**  \n",
        "   Feed the retrieved text into the model so it can generate a grounded, accurate answer\n"
      ],
      "metadata": {
        "id": "ZxWDmGOl31J0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QoTQu0Cf338b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Create a Gradio Interface\n",
        "\n",
        "Use **Gradio** to allow users to type in questions and receive answers from the chatbot.\n",
        "\n",
        "Use `gr.Interface()` with the following components:\n",
        "\n",
        "- `gr.Textbox()` as the **input**  \n",
        "- `gr.Textbox()` or `gr.HTML()` as the **output**  \n",
        "- Your custom **Q&A function** as the **backend logic**\n",
        "\n",
        "This interface will let users interact with your AI assistant through a simple, user-friendly web app.\n"
      ],
      "metadata": {
        "id": "SYFry5md4N_1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGdKQXcI4Ww_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Submit\n",
        "\n",
        "Make sure your final `.ipynb` notebook meets the following requirements:\n",
        "\n",
        "- 💬 Has **comments explaining each step** in your workflow  \n",
        "- 🔄 Shows the **full pipeline working** from document loading to answering questions  \n",
        "- 🧪 Includes **example questions and answers** about Nestlé’s HR policy\n",
        "\n",
        "Your notebook should clearly demonstrate your understanding of how to build an AI-powered Q&A assistant using OpenAI, LangChain, Chroma, and Gradio."
      ],
      "metadata": {
        "id": "8QSPQ1904XJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push to Git"
      ],
      "metadata": {
        "id": "ciIY2t4fDT3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔐 Set up Git identity (only needs to be done once per Colab session)\n",
        "#!git config --global user.name \"afarabee\"\n",
        "#!git config --global user.email \"aimee.farabee@crl.com\"\n"
      ],
      "metadata": {
        "id": "7frC-AS94ecK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone #################token#############"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G8Yxp-SDq2d",
        "outputId": "8559c8ee-92d4-437e-e0e8-ef5b98ce0d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai_powered_hr_assistant'...\n",
            "warning: You appear to have cloned an empty repository.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move your notebook into the repo folder\n",
        "#!mv /content/AI_Powered_HR_Assistant.ipynb /content/ai_powered_hr_assistant/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gFoOAJhFUda",
        "outputId": "f838f0b2-a4ca-436f-d36c-fc4fad5d0ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/AI_Powered_HR_Assistant.ipynb': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls /content/*.ipynb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuQhx3sDFgqG",
        "outputId": "59af3081-8f1a-4c99-ef95-7a709786184c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/*.ipynb': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fZ55LC4-Frky"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}