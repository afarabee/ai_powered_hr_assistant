{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNkRdkYe8I5ZP6vcQEVWnqi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afarabee/ai_powered_hr_assistant/blob/main/AI_Powered_HR_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Instructions\n",
        "**Crafting an AI-Powered HR Assistant: A Use Case for Nestl√©‚Äôs HR Policy Documents**\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Overview  \n",
        "The project aims to create a conversational chatbot that responds to user inquiries using PDF document information. It requires proficiency in:\n",
        "- Extracting and converting text into numerical vectors\n",
        "- Establishing an answer-finding mechanism\n",
        "- Designing a user-friendly chatbot interface with Gradio\n",
        "\n",
        "Additionally, the project emphasizes:\n",
        "- Structuring inquiries for clear communication\n",
        "- Deploying the chatbot for practical use\n",
        "- Guaranteeing the system's accessibility and efficiency in meeting user needs\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Instructions  \n",
        "- Review the learning materials and the Gradio documentation provided for the project  \n",
        "- Read the sections on **situation, task, action, and result** carefully to understand the assignment  \n",
        "- Complete and submit the assignment through the Learning Management System (LMS)  \n",
        "- Adhere closely to the provided guidelines, ensuring your submission contains all necessary analyses and interpretations  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Situation  \n",
        "As a developer, you have received the critical task of improving the operational efficiency of **Nestl√©'s Human Resources department**, a leading multinational corporation.\n",
        "\n",
        "Your toolkit includes:\n",
        "- Conversational AI technology\n",
        "- Python libraries\n",
        "- The powerful **GPT model from OpenAI**\n",
        "- The user-friendly **Gradio UI**\n",
        "\n",
        "Your mission is to integrate these advanced tools to **transform HR processes**, creating a more streamlined and efficient workflow within Nestl√©.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Task  \n",
        "Your task is to develop a **conversational chatbot** that answers queries about Nestl√©'s HR reports efficiently.\n",
        "\n",
        "You must:\n",
        "- Use **Python libraries**, **OpenAI's GPT model**, and **Gradio UI**\n",
        "- Create an interface that extracts and processes information from documents\n",
        "- Provide accurate responses to user queries through the chatbot\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Action Steps  \n",
        "\n",
        "- ‚úÖ Import essential tools and set up OpenAI's API environment  \n",
        "- ‚úÖ Load Nestl√©'s HR policy using `PyPDFLoader` and split it for easy processing  \n",
        "- ‚úÖ Create vector representations for text chunks using **ChromaDB** and **OpenAI's embeddings**  \n",
        "- ‚úÖ Build a question-answering system using the **GPT-3.5 Turbo model** to retrieve answers  \n",
        "- ‚úÖ Create a **prompt template** to guide the chatbot‚Äôs responses  \n",
        "- ‚úÖ Use **Gradio** to build a user-friendly chatbot interface for interaction and information retrieval  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Result  \n",
        "Upon completing this project, you will submit a `.ipynb` file demonstrating your ability to use advanced AI and machine learning technologies to develop a conversational chatbot.\n",
        "\n",
        "Your submission must include:\n",
        "- Setting up the programming environment  \n",
        "- Processing text documents  \n",
        "- Creating vector representations  \n",
        "- Building a question-answering system  \n",
        "- Designing a **Gradio interface** for effective interaction  \n",
        "\n",
        "Ensure the interface is **clear, usable, and accurate** in retrieving relevant information from Nestl√©‚Äôs HR policy.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "GiR0ng62svrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Requirements Breakdown\n",
        "\n",
        "You're building a smart **HR assistant** that can read the **Nestl√© HR policy PDF** and answer employee questions like:\n",
        "- ‚ÄúWhat is Nestl√©‚Äôs policy on promotions?‚Äù\n",
        "- ‚ÄúWhat does Nestl√© offer for employee training?‚Äù\n",
        "\n",
        "This involves:\n",
        "- üóÉÔ∏è Extracting text from the PDF  \n",
        "- üî¢ Converting text chunks into numerical format (embeddings)  \n",
        "- üß† Storing those embeddings in a retrievable way (Chroma DB)  \n",
        "- üí¨ Asking GPT to find answers using those chunks  \n",
        "- üñºÔ∏è Displaying this interaction using Gradio  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lZreg7yi2dOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Set Up Environment\n",
        "\n",
        "## Import essential tools and set up OpenAI API's environment\n",
        "\n",
        "- Install required packages (`openai`, `langchain`, `chromadb`, `PyPDFLoader`, `gradio`, etc.)  \n",
        "- Set your OpenAI API key securely using Colab Secrets  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ev-EMDgN12U7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OInV06TArYw8"
      },
      "outputs": [],
      "source": [
        "# Step 2: Setup environment\n",
        "\n",
        "# 2A: Install necessary libraries\n",
        "# openai ‚Üí to connect to GPT-3.5 Turbo via API\n",
        "# pypdf ‚Üí to split up Nestle's HR policy for easier processing, used by LangChain's PDF loader\n",
        "# chromadb ‚Üí use with OpenAI's Embeddings to create vector representations of chunks of text from the PDF that can be stored and searched for\n",
        "# gradio ‚Üí to build user-friendly chatbot interface, enabling interaction and information retrieval\n",
        "# langchain ‚Üí for handling document loading, splitting, and retrieval logic\n",
        "# tiktoken ‚Üí used internally by OpenAI for counting tokens (needed in retrieval chains)\n",
        "# langchain_community ‚Üí homes the Chroma integration\n",
        "# langchain-openai ‚Üí homes the OpenAI embeddings wrapper\n",
        "\n",
        "!pip install --quiet openai pypdf chromadb gradio langchain tiktoken langchain-community langchain-openai\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "Gw5cf2COwQ6D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2B: Access my OpenAI API key securely using Colab Secrets\n",
        "# Avoids typing my key manually or exposing it in the notebook\n",
        "\n",
        "from google.colab import userdata # access my stored keys in Colab\n",
        "from openai import OpenAI         # new OpenAI client class\n",
        "\n",
        "api_key = userdata.get(\"OpenAI\")  # securely fetch my key from Secrets\n",
        "client = OpenAI(api_key=api_key)  # pass it into the OpenAI client directly\n"
      ],
      "metadata": {
        "id": "P57h2l1Tu-4c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2C: Define get_completion function using the updated SDK and secure key\n",
        "\n",
        "from google.colab import userdata               # use Colab's secure key store\n",
        "from openai import OpenAI                       # import the new OpenAI client\n",
        "\n",
        "api_key = userdata.get(\"OpenAI\")                # securely fetch my key from Secrets\n",
        "client = OpenAI(api_key=api_key)                # build a working OpenAI client with that key\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):  # send the prompt using the new SDK style\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content   # extract and return the assistant's reply\n"
      ],
      "metadata": {
        "id": "pd2J2Ca32Hhv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Step 1 E2E\n",
        "\n",
        "get_completion(\"What is the purpose of a human resources policy?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "uxvonduXwpv8",
        "outputId": "28661633-6fbb-4390-bbd8-ed77c3674b47"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The purpose of a human resources policy is to provide guidelines and procedures for managing employees in a fair, consistent, and legally compliant manner. These policies help to ensure that employees are treated fairly, that their rights are protected, and that the organization operates in accordance with relevant laws and regulations. Additionally, human resources policies can help to promote a positive work environment, improve communication between employees and management, and support the overall goals and objectives of the organization.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load and Split the PDF\n",
        "\n",
        "## Load Nestle's HR Policy using PyPDFLoader and split for easy processing.\n",
        "\n",
        "Use LangChain‚Äôs PyPDFLoader to extract text from the Nestl√© HR policy. Then, break the content into smaller chunks.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "N66x6P7D1tSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3A: Install addtl requiredLangChain package\n",
        "#!pip install --quiet langchain langchain-community"
      ],
      "metadata": {
        "id": "UZLD3LJn89J6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3B: Import LangChain's PDF loader and text splitter\n",
        "#from langchain.document_loaders import PyPDFLoader\n",
        "#from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "ZSkhmKPT6sGU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "local file location: file:///C:/Users/aimee/OneDrive/Documents/AppliedGenAI/Building%20LLM%20Applications/Assessment_1/1728286846_the_nestle_hr_policy_pdf_2012.pdf"
      ],
      "metadata": {
        "id": "xSH5KLLK7K-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3A: Upload HR policy PDF to Colab environment to get the correct path\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "utaSAw047PNf",
        "outputId": "1b2efdcd-b7dd-4cad-c0c0-c8eeb7fb91dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-05495db1-e4f2-43d4-a55b-18843d054d6f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-05495db1-e4f2-43d4-a55b-18843d054d6f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1728286846_the_nestle_hr_policy_pdf_2012.pdf to 1728286846_the_nestle_hr_policy_pdf_2012 (1).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3B: Load the Nestl√© HR policy document and define the path\n",
        "pdf_path = \"/content/1728286846_the_nestle_hr_policy_pdf_2012.pdf\" ### update path if needed ###\n",
        "\n",
        "# Use PyPDFLoader from LangChain to extract PDF content\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "\n",
        "# Step 3B: Extract the document and return a list of LangChain Document objects\n",
        "# Each page will become a separate document object (1 per PDF pg)\n",
        "pages = loader.load()\n"
      ],
      "metadata": {
        "id": "PGV0_4d76z5_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Check how many pages were loaded\n",
        "print(f\"Loaded {len(pages)} pages from the PDF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJFd-ha9omXz",
        "outputId": "4b336141-a5a4-4a05-fdcb-8b63a903cf7f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 8 pages from the PDF.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Show characters 100-200 on page 2\n",
        "print(pages[2].page_content[100:200])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfG1JJvHxA4L",
        "outputId": "aa773171-f5a0-4d94-e2f4-9f914defbaa7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uccess and nothing can be \n",
            "achieved without their engagement. \n",
            "This document encompasses the guideli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Print the length of the first embedded document\n",
        "model_name = \"text-embedding-3-large\"\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-large\", #OpenAI embedding model to use\n",
        "    api_key=api_key                 #API key variable\n",
        ")\n",
        "text = pages[0].page_content\n",
        "embedding = embeddings.embed_query(text)\n",
        "print(len(embedding))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcXNf42vorFU",
        "outputId": "3661ef3c-1807-4fe0-a699-ffd171e98d87"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3C: Define how to chunk the text\n",
        "# I want chunks that are not too long, and that overlap slightly to preserve context\n",
        "text_splitter = RecursiveCharacterTextSplitter( #defines HOW to chunk the text\n",
        "    chunk_size=1000,      # max characters per chunk (adjustable)\n",
        "    chunk_overlap=100     # overlap to keep context from spilling over boundaries (adjustable)\n",
        ")\n",
        "\n",
        "# TEST: Text_splitter was created successfully\n",
        "print(type(text_splitter))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8OTAxM39saA",
        "outputId": "649490bf-da6c-413c-f450-61689071c2f6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_text_splitters.character.RecursiveCharacterTextSplitter'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3D: Split all pages into smaller chunks\n",
        "# This will give me a list of Document chunks, ready to be embedded later\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "# TEST: Preview the third chunk to see how it looks\n",
        "# Access the first element of the slice (which is the third chunk) and then its page_content\n",
        "print(chunks[2].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2FKO99s_gCV",
        "outputId": "5b31744a-a618-48b7-b51d-2ada853c2b4a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Nestl√© Human Resources Policy\n",
            "1\n",
            "At Nestl√©, we recognize that our employees \n",
            "are the key to our success and nothing can be \n",
            "achieved without their engagement. \n",
            "This document encompasses the guidelines \n",
            "which constitute a solid basis for effective Human \n",
            "Resources Management throughout the Nestl√© \n",
            "Group around the world. It explains to all Nestl√© \n",
            "employees the vision and mission of the Human \n",
            "Resources function and illustrates every aspect of \n",
            "the Nestl√© employee lifecycle. \n",
            "The Nestl√© Management and Leadership \n",
            "Principles inspire all the Nestl√© employees in their \n",
            "actions and in their dealings with others. The \n",
            "Corporate Business Principles refer to all the basic \n",
            "principles which Nestl√© endorses and subscribes \n",
            "to on a worldwide basis. Both these documents \n",
            "are the pillars on which the present policy has \n",
            "been built.\n",
            "The implementation of this policy will be \n",
            "inspired by sound judgement, compliance with \n",
            "local market laws and common sense, taking into\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf2fa396",
        "outputId": "a7bd3e20-5d13-4710-e540-ec202468bf90"
      },
      "source": [
        "# TEST: Print the content of the first 5 chunks\n",
        "for i, chunk in enumerate(chunks[:3]):  #adds a counter; returns pairs of (index, value) for each item in the list; \"for\" loop iterates through the pairs\n",
        "  print(f\"--- Chunk {i+1} ---\")         #prints a header for each chunk\n",
        "  print(chunk.page_content)             #prints text content of the current chunk\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chunk 1 ---\n",
            "Policy\n",
            "Mandatory\n",
            "September‚Äâ‚Äâ2012\n",
            "The Nestl√©  \n",
            "Human Resources Policy\n",
            "--- Chunk 2 ---\n",
            "Policy\n",
            "Mandatory\n",
            "September‚Äâ\n",
            "‚Äâ20\n",
            "12\n",
            "Issuing‚Äâdepartement\n",
            "Hum\n",
            "an Resources\n",
            "Target‚Äâaudience‚Äâ\n",
            "All\n",
            " employees\n",
            "Approver\n",
            "Executive Board, Nestl√© S.A.\n",
            "Repository\n",
            "All Nestl√© Principles and Policies, Standards and  \n",
            "Guidelines can be found in the Centre online repository at:  \n",
            "http://intranet.nestle.com/nestledocs\n",
            "Copyright\n",
            "‚Äâand‚Äâconfidentiality\n",
            "Al\n",
            "l rights belong to Nestec Ltd., Vevey, Switzerland.\n",
            "¬© 2012, Nestec Ltd.\n",
            "Design\n",
            "Nestec Ltd., Corporate Identity & Design,  \n",
            "Vevey, Switzerland\n",
            "Production\n",
            "brain‚Äôprint GmbH, Switzerland\n",
            "Paper\n",
            "This report is printed on BVS, a paper produced  \n",
            "from well-managed forests and other controlled sources  \n",
            "certified by the Forest Stewardship Council (FSC).\n",
            "--- Chunk 3 ---\n",
            "The Nestl√© Human Resources Policy\n",
            "1\n",
            "At Nestl√©, we recognize that our employees \n",
            "are the key to our success and nothing can be \n",
            "achieved without their engagement. \n",
            "This document encompasses the guidelines \n",
            "which constitute a solid basis for effective Human \n",
            "Resources Management throughout the Nestl√© \n",
            "Group around the world. It explains to all Nestl√© \n",
            "employees the vision and mission of the Human \n",
            "Resources function and illustrates every aspect of \n",
            "the Nestl√© employee lifecycle. \n",
            "The Nestl√© Management and Leadership \n",
            "Principles inspire all the Nestl√© employees in their \n",
            "actions and in their dealings with others. The \n",
            "Corporate Business Principles refer to all the basic \n",
            "principles which Nestl√© endorses and subscribes \n",
            "to on a worldwide basis. Both these documents \n",
            "are the pillars on which the present policy has \n",
            "been built.\n",
            "The implementation of this policy will be \n",
            "inspired by sound judgement, compliance with \n",
            "local market laws and common sense, taking into\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Create Embeddings & Store Them in Chroma\n",
        "\n",
        "##Create vector representations for text chunks using Chroma dB and OpenAI's embeddings.\n",
        "\n",
        "Use OpenAI‚Äôs embedding model (such as `text-embedding-3-large`) to:\n",
        "\n",
        "1. üî¢ **Convert each chunk into a numerical vector**  \n",
        "   This transforms the text into a format that can be compared mathematically for similarity.\n",
        "\n",
        "2. üß† **Store the vectors in ChromaDB**  \n",
        "   This allows the system to **retrieve the most relevant chunks** when users ask questions, based on vector similarity.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LrdyzDe83v0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4a - Store my Chroma DB in GDrive so it persists across sessions\n",
        "\n",
        "Created the folder\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs(persist_directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "vZy6-CFNvWqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount my Google Drive so Colab can read/write to it.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os # Import the os module\n",
        "\n",
        "# tells the rest of the code where to save/load the DB\n",
        "persist_directory = \"/content/drive/MyDrive/ai_powered_hr_assistant/chroma_nestle\"\n",
        "\n",
        "# Validate that the path is ready.\n",
        "print(\"Chroma DB folder set to:\", persist_directory)\n",
        "print(\"Folder exists?\", os.path.isdir(persist_directory))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qQUCv_WvIOc",
        "outputId": "68e39c4c-9c1b-42d9-be96-8288d25fb313"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Chroma DB folder set to: /content/drive/MyDrive/ai_powered_hr_assistant/chroma_nestle\n",
            "Folder exists? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4b: Build the embeddings object"
      ],
      "metadata": {
        "id": "BkQdcJA20cpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2TWPTg5bxi5",
        "outputId": "22754d0b-6225-4adb-9758-4166c56e46db"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-chroma in /usr/local/lib/python3.11/dist-packages (0.2.5)\n",
            "Requirement already satisfied: langchain-core>=0.3.70 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma) (0.3.74)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma) (2.0.2)\n",
            "Requirement already satisfied: chromadb>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma) (1.0.20)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.14.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.21.4)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (3.11.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.25.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain-chroma) (0.4.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain-chroma) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain-chroma) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.70->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.27.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.10)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain-chroma) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain-chroma) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.9->langchain-chroma) (0.57b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (1.1.7)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma"
      ],
      "metadata": {
        "id": "8XfElFl-b01C"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed + persist (OpenAI‚Äôs current recs: text-embedding-3-small/large)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=api_key) #load API key again if needed\n"
      ],
      "metadata": {
        "id": "1DMZfs5Ovyx8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the embeddings model directly during Chroma initialization\n",
        "# Docs are auto persisted in new versions of Chroma, no need for vectordb.persist()\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "5JEuvjp-38Wr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the vector store into a Retriever object\n",
        "# The Chroma vector store object (vectordb) has a method to easily convert it into a retriever\n",
        "\n",
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "oP9_K3yXr_ws"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "246d8084",
        "outputId": "c3bb417f-2cd5-45ba-d8c3-48367fe35d7c"
      },
      "source": [
        "# Test the vector database by performing a similarity search\n",
        "query = \"What is Nestl√©'s policy on employee training?\"\n",
        "# Use k= to retrieve only the top n most relevant documents\n",
        "docs = vectordb.similarity_search(query, k=3)\n",
        "\n",
        "# Print the content of the retrieved documents to see if they are relevant\n",
        "print(f\"Retrieved {len(docs)} documents:\")\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"--- Retrieved Document {i+1} ---\")\n",
        "    print(doc.page_content)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 3 documents:\n",
            "--- Retrieved Document 1 ---\n",
            "The Nestl√© Human Resources Policy\n",
            "4\n",
            "Learning is part of the Company culture.\n",
            "Employees at all levels are systematically \n",
            "encouraged to consider how they upgrade their \n",
            "knowledge and skills.\n",
            "The Company determines training and deve-\n",
            "lopment priorities. The responsibility for turning \n",
            "these into actions is shared between employees, \n",
            "line managers and the Human Resources. \n",
            "Experience and on-the-job training are the \n",
            "primary source of learning. Managers are \n",
            "responsible for guiding and coaching employees \n",
            "to succeed in their current positions.  \n",
            "Nestl√© employees understand the importance \n",
            "of continuous improvement, as well as sharing \n",
            "knowledge and ideas freely with others. Practices \n",
            "such as lateral professional development, \n",
            "extension of responsibilities, and cross functional \n",
            "teams are encouraged to acquire additional skills, \n",
            "enrich job content and widen accountability.\n",
            "Nestl√© also offers a comprehensive range of \n",
            "training activities and methodologies to support\n",
            "\n",
            "\n",
            "--- Retrieved Document 2 ---\n",
            "The Nestl√© Human Resources Policy\n",
            "4\n",
            "Learning is part of the Company culture.\n",
            "Employees at all levels are systematically \n",
            "encouraged to consider how they upgrade their \n",
            "knowledge and skills.\n",
            "The Company determines training and deve-\n",
            "lopment priorities. The responsibility for turning \n",
            "these into actions is shared between employees, \n",
            "line managers and the Human Resources. \n",
            "Experience and on-the-job training are the \n",
            "primary source of learning. Managers are \n",
            "responsible for guiding and coaching employees \n",
            "to succeed in their current positions.  \n",
            "Nestl√© employees understand the importance \n",
            "of continuous improvement, as well as sharing \n",
            "knowledge and ideas freely with others. Practices \n",
            "such as lateral professional development, \n",
            "extension of responsibilities, and cross functional \n",
            "teams are encouraged to acquire additional skills, \n",
            "enrich job content and widen accountability.\n",
            "Nestl√© also offers a comprehensive range of \n",
            "training activities and methodologies to support\n",
            "\n",
            "\n",
            "--- Retrieved Document 3 ---\n",
            "The Nestl√© Human Resources Policy\n",
            "4\n",
            "Learning is part of the Company culture.\n",
            "Employees at all levels are systematically \n",
            "encouraged to consider how they upgrade their \n",
            "knowledge and skills.\n",
            "The Company determines training and deve-\n",
            "lopment priorities. The responsibility for turning \n",
            "these into actions is shared between employees, \n",
            "line managers and the Human Resources. \n",
            "Experience and on-the-job training are the \n",
            "primary source of learning. Managers are \n",
            "responsible for guiding and coaching employees \n",
            "to succeed in their current positions.  \n",
            "Nestl√© employees understand the importance \n",
            "of continuous improvement, as well as sharing \n",
            "knowledge and ideas freely with others. Practices \n",
            "such as lateral professional development, \n",
            "extension of responsibilities, and cross functional \n",
            "teams are encouraged to acquire additional skills, \n",
            "enrich job content and widen accountability.\n",
            "Nestl√© also offers a comprehensive range of \n",
            "training activities and methodologies to support\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Build the Q&A Pipeline\n",
        "\n",
        "##*Build a question-answering system using the GPT-3.5 Turbo model to retrieve answers from text chunks.*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Build a RetrievalQA chain that uses:\n",
        " - my prompt template (QA_CHAIN_PROMPT)\n",
        " - my retriever (built from the Chroma vectorstore)\n",
        " - an LLM (ChatOpenAI) with temperature=0 for consistent answers\n",
        "\n",
        "----\n",
        "Use **LangChain‚Äôs `RetrievalQA`** (or a similar approach) to create an intelligent question-answering flow.\n",
        "\n",
        "The pipeline should:\n",
        "\n",
        "1. **Take the user's question**  \n",
        "   Accept natural language input from the user (e.g., ‚ÄúWhat is Nestl√©'s policy on promotions?‚Äù)\n",
        "\n",
        "2. **Retrieve the most relevant chunks from ChromaDB**  \n",
        "   Use similarity search to find the document sections most related to the question\n",
        "\n",
        "3. **Pass those chunks to GPT-3.5 Turbo as context**  \n",
        "   Feed the retrieved text into the model so it can generate a grounded, accurate answer"
      ],
      "metadata": {
        "id": "ZxWDmGOl31J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5.1: Import the LangChain OpenAI chat wrapper\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "VZqgBcVv4RoW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5.2: Initialize the language model (GPT-3.5 Turbo); Create the LLM object\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    api_key=api_key\n",
        ")"
      ],
      "metadata": {
        "id": "2w-Ra69ip1pK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa89bcbe",
        "outputId": "84fc5efd-b03f-41b7-ffe1-a79246de26ca"
      },
      "source": [
        "# TEST the LLM connectivity\n",
        "print(llm.invoke(\"Hello, how are you?\"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 13, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6HbW0g8oSB0tLAiGJTfCGhousAtN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--1895a388-2c97-4520-bc30-2ce2b37d602c-0' usage_metadata={'input_tokens': 13, 'output_tokens': 33, 'total_tokens': 46, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e62a7eec"
      },
      "source": [
        "#Step 5.3: Assemble the retriever\n",
        "from langchain.chains import RetrievalQA\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Retrieve relevant documents from the vectordb using similarity search with RetrievalQA\n",
        "\n",
        "query = \"What ensures the success of Nestle as a company?\"\n",
        "docs = vectordb.similarity_search(query)\n",
        "\n",
        "# Retrieve only the top n most relevant documents\n",
        "docs = vectordb.similarity_search(query, k=3)\n",
        "\n",
        "# Print the content of the retrieved documents to see if they are relevant\n",
        "print(f\"Retrieved {len(docs)} documents:\")\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"--- Retrieved Document {i+1} ---\")\n",
        "    print(doc.page_content)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "QoTQu0Cf338b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a443d1-bb23-4c55-f3ce-9ff2f69d4395"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 3 documents:\n",
            "--- Retrieved Document 1 ---\n",
            "of a global company with the creativity and \n",
            "knowledge of a local business. As a result, people \n",
            "can have far-reaching influence every day and \n",
            "explore their full long-term potential, propelled by \n",
            "continual support and a collaborative approach by \n",
            "line managers and employees.\n",
            "Corporate policy: \n",
            "Nestl√© on the Move\n",
            " A flexible and dynamic organisation\n",
            "\n",
            "\n",
            "--- Retrieved Document 2 ---\n",
            "of a global company with the creativity and \n",
            "knowledge of a local business. As a result, people \n",
            "can have far-reaching influence every day and \n",
            "explore their full long-term potential, propelled by \n",
            "continual support and a collaborative approach by \n",
            "line managers and employees.\n",
            "Corporate policy: \n",
            "Nestl√© on the Move\n",
            " A flexible and dynamic organisation\n",
            "\n",
            "\n",
            "--- Retrieved Document 3 ---\n",
            "of a global company with the creativity and \n",
            "knowledge of a local business. As a result, people \n",
            "can have far-reaching influence every day and \n",
            "explore their full long-term potential, propelled by \n",
            "continual support and a collaborative approach by \n",
            "line managers and employees.\n",
            "Corporate policy: \n",
            "Nestl√© on the Move\n",
            " A flexible and dynamic organisation\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5.4: Create a Prompt Template\n",
        "\n",
        "# prompt_template: tell it what to use (the context), what not to do, and limit output length\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer based on the provided context, please respond with \"Sorry, I could not find an answer. Would you like to try a different question?\". Do not try to make up an answer.\n",
        "Use three sentences maximum.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zkmr4GqSp-MQ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###Breakdown of Step 5.4 \"Create a Prompt Template\"\n",
        "\n",
        "####1. The instructions to the LLM:\n",
        "```\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum.\n",
        "\n",
        "```\n",
        "These are system-like instructions baked into the prompt to guide how the LLM should behave.\n",
        "\n",
        "They:\n",
        "\n",
        "Tell it what to use (the context we‚Äôll give it)\n",
        "\n",
        "Tell it what not to do (no guessing / hallucinating)\n",
        "\n",
        "Put a limit on output length (max three sentences)\n",
        "\n",
        "\n",
        "####2. {context} placeholder\n",
        "\n",
        "```\n",
        "{context}\n",
        "\n",
        "```\n",
        "This is a placeholder in a Python format string.\n",
        "\n",
        "Later, you‚Äôll insert the actual retrieved text chunks here:\n",
        "\n",
        "\n",
        "```\n",
        "prompt = prompt_template.format(context=\"policy text...\", question=\"What is Nestl√©'s policy on training?\")\n",
        "\n",
        "```\n",
        "The curly braces {} get replaced with real values by .format().\n",
        "\n",
        "####3. The question and answer section\n",
        "\n",
        "\n",
        "```\n",
        "Question: {question}\n",
        "Answer:\n",
        "\n",
        "```\n",
        "{question} is another placeholder where you‚Äôll insert the actual user‚Äôs question.\n",
        "\n",
        "The Answer: line is left blank so the LLM will write the answer directly after it.\n",
        "\n",
        "\n",
        "####4. Putting it together\n",
        "When you run:\n",
        "```\n",
        "    prompt = prompt_template.format(\n",
        "        context=\"Nestl√© provides a wide range of training programs...\",\n",
        "        question=\"What is Nestl√©'s policy on training?\"\n",
        "    )\n",
        "    print(prompt)\n",
        "```\n",
        "\n",
        "You get:\n",
        "```\n",
        "    Use the following pieces of context to answer the question at the end.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    Use three sentences maximum.\n",
        "\n",
        "    Nestl√© provides a wide range of training programs...\n",
        "\n",
        "    Question: What is Nestl√©'s policy on training?\n",
        "    Answer:\n",
        "```\n",
        "\n",
        "You can reuse the same template for every question.\n",
        "\n",
        "The {context} gets filled dynamically with retrieved chunks from Chroma.\n",
        "\n",
        "The {question} gets filled with the user‚Äôs query.\n",
        "\n",
        "This keeps your LLM grounded in the right document and avoids hallucination.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mseZCDSf7XAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5.5: Turn my text template into a PromptTemplate object that L.C. can use\n",
        "# L.C. will dynamically construct the final prompt sent to GPT-3.5 Turbo during the Q&A process.\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template) #takes raw string input and converts into PromptTemplate that L.C. can work with\n",
        "\n",
        "# This method parses the string, identifies the placeholders, & sets up the structure so when P.T. is used later, L.C. knows where to inject the actual document context and users' question.\n",
        "# Required b/c L.C. chains, like RetrievalQA, are designed to work w/ structured objects"
      ],
      "metadata": {
        "id": "W_PEHMDXwO-7"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5.6: Create the RetrievalQA chain & assign it to the qa_chain variable\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    # initialized model object = GPT-3.5 Turbo with temperature=0\n",
        "    llm=llm,\n",
        "    # passes retriever object (created from vectordb); tells the chain where to get the relevant document chunks from when a query comes in\n",
        "    retriever=retriever,\n",
        "    # Optional: returns the chunks used to generate the answer\n",
        "    return_source_documents=True,\n",
        "    # RetrievalQA uses chain type=\"stuff,\" by default. This part tells the \"stuff\" chain to use the custom QA_CHAIN_PROMPT as the template for the prompt it constructs.\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT} # Pass the prompt template to the chain. allows you to pass additional parameters to the underlying chain type being used by RetrievalQA\n",
        ")"
      ],
      "metadata": {
        "id": "Fb8JmkidwRwu"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST the chain by asking a question\n",
        "query = \"What is the HR structure at Nestle?\"\n",
        "response = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "# Print the Question and Answer\n",
        "print(\"Question:\", query)\n",
        "print(\"Answer:\", response['result'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo9Tq1imwVIg",
        "outputId": "e5a4bc29-204e-4b7b-c753-ab0aed355d64"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the HR structure at Nestle?\n",
            "Answer: The HR structure at Nestle is based on three dedicated areas: Centres of Expertise, Business Partners, and Employee Services. Each area provides specialized services, deploys HR strategies within specific businesses, and performs transactional activities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Print source documents\n",
        "print(\"\\nSource Documents:\")\n",
        "for doc in response['source_documents']:\n",
        "     print(doc.page_content[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfh7txbx2QW9",
        "outputId": "2955db5f-20a2-4a97-c258-b3aa13c48e07"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Source Documents:\n",
            "HR has adopted a streamlined approach to \n",
            "ensuring functional leadership and the highest \n",
            "level of focus, clarity, and efficiency. Our structure \n",
            "is based on three dedicated areas which provide \n",
            "speci\n",
            "HR has adopted a streamlined approach to \n",
            "ensuring functional leadership and the highest \n",
            "level of focus, clarity, and efficiency. Our structure \n",
            "is based on three dedicated areas which provide \n",
            "speci\n",
            "HR has adopted a streamlined approach to \n",
            "ensuring functional leadership and the highest \n",
            "level of focus, clarity, and efficiency. Our structure \n",
            "is based on three dedicated areas which provide \n",
            "speci\n",
            "HR has adopted a streamlined approach to \n",
            "ensuring functional leadership and the highest \n",
            "level of focus, clarity, and efficiency. Our structure \n",
            "is based on three dedicated areas which provide \n",
            "speci\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST with a different question\n",
        "query = \"What is the mission of HR Managers at Nestl√©?\"\n",
        "response = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "# Print the Question and Answer\n",
        "print(\"Question:\", query)\n",
        "print(\"Answer:\", response['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj3EizZo2p-n",
        "outputId": "f5718c75-27a1-4819-9e39-d4ebc3dcf370"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the mission of HR Managers at Nestl√©?\n",
            "Answer: The mission of HR managers at Nestl√© is to provide professional guidance to line managers in order to deliver superior business results by optimizing the performance of their people while ensuring exemplary working conditions. They aim to establish business needs and corresponding people requirements, empowering line managers to build and sustain an environment where employees have a sense of personal commitment to their work and give their best for the company's success. With a 'Nestl√© in the Market' approach, HR ensures functional leadership and the highest standards within the organization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Create a Gradio Interface\n",
        "\n",
        "## *Use Gradio to build a user-friendly chatbot interface, enabling interaction and information retrieval.*\n",
        "\n",
        "Allow users to type in questions and receive answers from the chatbot.\n",
        "\n"
      ],
      "metadata": {
        "id": "SYFry5md4N_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6.1: Install gradio (safe to re-run; does nothing if it's already there)\n",
        "!pip install --quiet gradio"
      ],
      "metadata": {
        "id": "dGdKQXcI4Ww_"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6.2: Import the library to build the web UI and test it\n",
        "import gradio as gr\n",
        "\n",
        "def greet(name, intensity):\n",
        "    return \"Hello \" * intensity + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=greet,\n",
        "    inputs=[\"text\", \"slider\"],\n",
        "    outputs=[\"text\"],\n",
        ")\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "eIhfMJyFEclq",
        "outputId": "1ffc85bd-37d5-417e-a34e-9c5bee491b09"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://12d32af22459f7e18e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://12d32af22459f7e18e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6.3: Print the version to confirm it's available in this runtime\n",
        "print(\"Gradio version:\", gr.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbIzdXW_EkMl",
        "outputId": "9198b3e3-9794-48b8-f5df-634ed7b5b0ac"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio version: 5.42.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6.4: Backend function that takes the user's question (string), sends to RetrievalQA (qa_chain) chain and returns an answer (string)\n",
        "def answer_question(user_question: str) -> str:\n",
        "    \"\"\"\n",
        "    Backend:\n",
        "    - Accept a question string from the UI\n",
        "    - Use the RetrievalQA chain (qa_chain) to get an answer\n",
        "    - Return a clean text response for display in Gradio\n",
        "    \"\"\"\n",
        "\n",
        "    # TEST: make sure qa_chain exists (built in Step 5)\n",
        "    assert 'qa_chain' in globals(), \"qa_chain not found. Build the RetrievalQA chain before launching Gradio.\"\n",
        "\n",
        "\n",
        "    # Handle Empty input\n",
        "    if not user_question or not user_question.strip():\n",
        "        return \"Please enter a question about the Nestl√© HR policy.\"\n",
        "\n",
        "\n",
        "    # Process the question using the QA chain\n",
        "    # Assuming qa_chain is defined globally or passed in\n",
        "    response = qa_chain.invoke({\"query\": user_question})\n",
        "\n",
        "\n",
        "    # Extract the answer text from the response, with default message if key isn't found or is None\n",
        "    answer_text = response.get('result', 'Sorry, I could not find an answer. Would you like to try a different question?')\n",
        "\n",
        "\n",
        "    # Return final message to the UI\n",
        "    return answer_text"
      ],
      "metadata": {
        "id": "TDZVypx_Etki"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: call the backend directly (bypassing Gradio) to verify it returns an answer\n",
        "test_q = \"What is the Policy on Conditions of Work and Employment?\"\n",
        "\n",
        "# Call the function to print the answer\n",
        "print(answer_question(test_q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJsd7fi4FrYR",
        "outputId": "02a2fe6d-33a7-43b3-9d4c-998585d83caf"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Policy on Conditions of Work and Employment outlines the guidelines and regulations related to employment and working conditions within the organization. It serves as a framework for ensuring a fair and respectful environment for employees. HR plays a crucial role in implementing and upholding this policy to support the well-being of the workforce.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio UI to call the backend function and launch the Gradio app\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# Create input + output widgets\n",
        "#   Small text box to type a question\n",
        "#   Larger text box to display answer\n",
        "question_box = gr.Textbox(\n",
        "    label=\"Ask a question about the Nestl√© HR policy\",\n",
        "    placeholder=\"e.g., What does Nestl√© say about training and development?\",\n",
        "    lines=2\n",
        ")\n",
        "answer_box = gr.Textbox(\n",
        "    label=\"Answer\",\n",
        "    lines=10\n",
        ")\n",
        "\n",
        "# Wrap backend function + widgets into a simple interface\n",
        "demo = gr.Interface(\n",
        "    fn=answer_question,     # function defined in step 6.4\n",
        "    inputs=question_box,    # input component(s)\n",
        "    outputs=answer_box,     # output component(s)\n",
        "    title=\"Nestl√© HR Assistant\",\n",
        "    description=\"Type your question below and I‚Äôll do my best to help.\"\n",
        ")\n",
        "\n",
        "# Launch the app (share=True gives me a temporary public link I can submit/test)\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "Ibe2z4qWKkpu",
        "outputId": "9bc0a29f-6a16-469f-adb3-2ca6502d3f57"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://afc651e1cc7a92ce0e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://afc651e1cc7a92ce0e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Submit\n",
        "\n",
        "Make sure your final `.ipynb` notebook meets the following requirements:\n",
        "\n",
        "- üí¨ Has **comments explaining each step** in your workflow  \n",
        "- üîÑ Shows the **full pipeline working** from document loading to answering questions  \n",
        "- üß™ Includes **example questions and answers** about Nestl√©‚Äôs HR policy\n",
        "\n",
        "Your notebook should clearly demonstrate your understanding of how to build an AI-powered Q&A assistant using OpenAI, LangChain, Chroma, and Gradio."
      ],
      "metadata": {
        "id": "8QSPQ1904XJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push to Git"
      ],
      "metadata": {
        "id": "ciIY2t4fDT3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîê Set up Git identity (only needs to be done once per Colab session)\n",
        "#!git config --global user.name \"afarabee\"\n",
        "#!git config --global user.email \"aimee.farabee@crl.com\"\n"
      ],
      "metadata": {
        "id": "7frC-AS94ecK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone #################token#############"
      ],
      "metadata": {
        "id": "5G8Yxp-SDq2d"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move your notebook into the repo folder\n",
        "#!mv /content/AI_Powered_HR_Assistant.ipynb /content/ai_powered_hr_assistant/\n"
      ],
      "metadata": {
        "id": "3gFoOAJhFUda"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls /content/*.ipynb\n"
      ],
      "metadata": {
        "id": "tuQhx3sDFgqG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fZ55LC4-Frky"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}